import os
import sys
import json
import argparse
import time
import subprocess
import functools
import numpy as np
from typing import Optional, Tuple
from tqdm import tqdm
import traceback # Added for better error reporting
import signal # <<< MODIFIED: Import signal >>>
from torch.compiler import cudagraph_mark_step_begin # <<< ADD THIS LINE
import math # For ceil in dataset chunking (though that script is separate)

# <<< MODIFIED: Set Tokenizers Parallelism Environment Variable >>>
# Set this early, before tokenizers might be implicitly loaded by other imports
# Setting to "true" forces parallelism despite potential fork issues (use with caution)
# Setting to "false" explicitly disables parallelism in worker processes (safer, suppresses warning)
# Only run this environment setup logic if on Windows
if os.name == 'nt':
    CACHE_FILE = 'vcvars_path.cache.txt'
    vcvars_path = None

    # 1. Try to load the path from the cache file
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r') as f:
                cached_path = f.read().strip()
            if os.path.exists(cached_path):
                print(f"INFO: Found cached vcvars64.bat path: {cached_path}")
                vcvars_path = cached_path
            else:
                print(f"INFO: Cached path '{cached_path}' no longer exists. Will try auto-detect.")
        except Exception as e:
            print(f"WARNING: Could not read cache file. Will try auto-detect. Error: {e}")

    # 2. If no valid path from cache, try auto-detection
    if vcvars_path is None:
        print("INFO: No cached vcvars path. Attempting auto-detection...")
        # Check standard paths for vswhere.exe
        vswhere_path_progfiles_x86 = os.path.expandvars(r"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe")
        vswhere_path_progfiles = os.path.expandvars(r"%ProgramFiles%\Microsoft Visual Studio\Installer\vswhere.exe")
        
        vswhere_path = None
        if os.path.exists(vswhere_path_progfiles_x86):
            vswhere_path = vswhere_path_progfiles_x86
        elif os.path.exists(vswhere_path_progfiles):
            vswhere_path = vswhere_path_progfiles
            
        if vswhere_path:
            try:
                # <<< MODIFICATION: Use vswhere to find vcvars64.bat directly >>>
                # This is more robust than finding the installationPath and guessing.
                # It asks for the path to 'vcvars64.bat' from the latest product
                # that *requires* the C++ tools.
                print("INFO: vswhere.exe found. Querying directly for vcvars64.bat...")
                cmd = [
                    vswhere_path,
                    "-latest",
                    "-products", "*",
                    "-requires", "Microsoft.VisualStudio.Component.VC.Tools.x86.x64",
                    "-find", r"VC\Auxiliary\Build\vcvars64.bat",
                    "-nologo"
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, check=True, encoding='utf-8')
                found_path = result.stdout.strip()

                if found_path and os.path.exists(found_path):
                    vcvars_path = found_path
                    print(f"✅ INFO: vswhere.exe successfully found vcvars64.bat at: {vcvars_path}")
                else:
                    print(f"INFO: vswhere.exe ran but did not return a valid path.")
                    print(f"   (stdout: {result.stdout})")

            except Exception as e:
                print(f"WARNING: 'vswhere.exe' direct find failed. Will fall back to manual prompt. Error: {e}")
                # Note: stdout/stderr might be in e.args if check=True failed
                if hasattr(e, 'stdout'): print(f"   (stdout: {e.stdout})")
                if hasattr(e, 'stderr'): print(f"   (stderr: {e.stderr})")
        else:
            print(f"INFO: 'vswhere.exe' not found in standard locations. Will fall back to manual prompt.")

    # 3. If auto-detection failed, fall back to user prompt
    if vcvars_path is None:
        print("---" * 20)
        print("Microsoft Visual C++ (MSVC) 64-bit Compiler Setup (Manual Fallback)")
        print("---" * 20)
        print("Could not automatically find your 64-bit C++ compiler.")
        print("To use 'torch.compile' (e.g., with --gradient-checkpointing), this script")
        print("needs to find your 'vcvars64.bat' file.")
        print("\nIt is usually located in a path like:")
        print(r"  C:\Program Files\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat")
        print(r"  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\Build\vcvars64.bat")
        print("---" * 20)
        
        try:
            user_path = input("Enter the full path to vcvars64.bat (or press Enter to skip): ").strip()

            if user_path and os.path.exists(user_path):
                vcvars_path = user_path
                print("INFO: Path validated.")
            elif user_path: # User provided a path, but it's invalid
                print(f"WARNING: Path '{user_path}' not found or is invalid.")
                print("         Skipping environment setup. torch.compile may fail.")
            else: # User pressed Enter
                print("INFO: Skipping 64-bit environment setup.")
                print("         'torch.compile' may fail unless you are in the correct dev terminal.")
        
        except EOFError:
             print("\nINFO: No input received. Skipping 64-bit environment setup.")
        
        print("---" * 20)

    # 4. If we have a valid path (from cache, auto-detect, or input), run it and set the environment
    if vcvars_path:
        # Cache the valid path for next time, regardless of how we found it
        try:
            with open(CACHE_FILE, 'w') as f:
                f.write(vcvars_path)
            print(f"INFO: 64-bit compiler path cached to '{CACHE_FILE}' for future runs.")
        except Exception as e:
            print(f"WARNING: Could not write cache file. You may be asked again. Error: {e}")

        print(f"INFO: Loading 64-bit MSVC environment from '{vcvars_path}'...")
        try:
            python_exe = sys.executable
            vcvars_dir = os.path.dirname(vcvars_path)
            vcvarsall_path = os.path.join(vcvars_dir, "vcvarsall.bat")

            if not os.path.exists(vcvarsall_path):
                print(f"❌ ERROR: 'vcvarsall.bat' not found in the same directory as 'vcvars64.bat'.")
                print(f"  Looked for: {vcvarsall_path}")
                raise FileNotFoundError("vcvarsall.bat not found")

            # This command runs vcvarsall.bat x64, which sets up the env,
            # then (&&) runs a python one-liner (using the *exact* python.exe path)
            # to print that new environment as a JSON string.
            # --- FIX: Add >NUL 2>&1 to redirect stdout and stderr ---
            cmd = f'"{vcvarsall_path}" x64 >NUL 2>&1 && echo ^"---ENV-JSON-START---^" && "{python_exe}" -c "import json, os; print(json.dumps(dict(os.environ)))"'
            
            # Hide the command window
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                shell=True,
                check=True, # This will raise CalledProcessError if it fails
                startupinfo=startupinfo,
                encoding='utf-8',
                errors='ignore',
                cwd=vcvars_dir # Set CWD just in case
            )
            
            json_start = result.stdout.find("---ENV-JSON-START---")
            if json_start != -1:
                # --- FIX: More robust JSON finding to skip control characters ---
                raw_output_after_marker = result.stdout[json_start + len("---ENV-JSON-START---"):]
                
                # Find the first '{'
                json_obj_start = raw_output_after_marker.find('{')
                # Find the last '}'
                json_obj_end = raw_output_after_marker.rfind('}')
                
                if json_obj_start != -1 and json_obj_end != -1 and json_obj_end > json_obj_start:
                    json_str = raw_output_after_marker[json_obj_start : json_obj_end + 1]
                    
                    # Now try to parse this cleaned string
                    try:
                        env_vars = json.loads(json_str)
                        
                        # ==========================================================
                        # --- ❗️❗️❗️ BEGIN KEY FIX ❗️❗️❗️ ---
                        # ==========================================================
                        # We must use env_vars.get() to retrieve the new values
                        # and only assign them to os.environ if they are not None.
                        # This avoids the KeyError: 'LIB' when os.environ['LIB']
                        # is used as a fallback but doesn't exist.

                        new_path = env_vars.get('PATH')
                        new_lib = env_vars.get('LIB')
                        new_include = env_vars.get('INCLUDE')
                        
                        if new_path is not None:
                            os.environ['PATH'] = new_path
                        else:
                            print("WARNING: 'PATH' not found in vcvarsall.bat output.")
                            
                        if new_lib is not None:
                            os.environ['LIB'] = new_lib
                        else:
                            # This was the variable causing the KeyError.
                            # Set it to an empty string if not provided by vcvarsall.
                            print("INFO: 'LIB' not found in vcvarsall.bat output. Setting to empty string.")
                            os.environ['LIB'] = "" 
                            
                        if new_include is not None:
                            os.environ['INCLUDE'] = new_include
                        else:
                            # Also set INCLUDE to empty string just in case.
                            print("INFO: 'INCLUDE' not found in vcvarsall.bat output. Setting to empty string.")
                            os.environ['INCLUDE'] = ""
                        
                        # ==========================================================
                        # --- ❗️❗️❗️ END KEY FIX ❗️❗️❗️ ---
                        # ==========================================================

                        print("✅ INFO: 64-bit MSVC environment loaded successfully.")
                    except json.JSONDecodeError as json_e:
                        print(f"❌ ERROR: Failed to parse the environment JSON. {json_e}")
                        print("     --- Raw JSON String (cleaned) ---")
                        print(json_str[:500] + "...") # Print start of what was parsed
                        print("     --- Full stdout after marker ---")
                        print(raw_output_after_marker[:500] + "...")
                else:
                    print(f"WARNING: Could not find valid JSON object markers '{{' and '}}' after '---ENV-JSON-START---'.")
                    print("STDOUT after marker:", raw_output_after_marker)
            else:
                print(f"WARNING: Could not find JSON marker in vcvarsall.bat output. Compiler might still fail.")
                print("STDOUT:", result.stdout)
                print("STDERR:", result.stderr)

        except subprocess.CalledProcessError as e:
            print(f"❌ ERROR: Failed to run vcvarsall.bat. torch.compile may fail.")
            print(f"   Return Code: {e.returncode}")
            print("   --- STDOUT ---")
            print(e.stdout if e.stdout else "<No stdout>")
            print("   --- STDERR ---")
            print(e.stderr if e.stderr else "<No stderr>")
            print("   --- End Output ---")
            
            # If the cached path failed, delete it so we ask again next time
            if os.path.exists(CACHE_FILE):
                print("INFO: Deleting bad cached path...")
                try:
                    os.remove(CACHE_FILE)
                except Exception as del_e:
                    print(f"WARNING: Could not delete cache file: {del_e}")
                    
        except Exception as e:
            # Catch other errors like file-not-found for subprocess itself or JSONDecodeError
            print(f"❌ ERROR: An unexpected error occurred while trying to run vcvarsall.bat.")
            print(f"   Error: {e}")
            print(f"   Error Type: {type(e)}") # Added type for debugging
            traceback.print_exc(limit=2) # Print traceback for this
        print("---" * 20)

os.environ["TOKENIZERS_PARALLELISM"] = "true" 

import torch
import torch.nn as nn
import torch.nn.functional as F
# <<< MODIFIED: Import IterableDataset and Dataset >>>
from torch.utils.data import Dataset, DataLoader, IterableDataset
from transformers import AutoTokenizer
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.serialization import safe_globals # May not be needed if not using legacy save/load
# <<< NEW: Import gradient checkpointing >>>
from torch.utils.checkpoint import checkpoint
# <<< END >>>


# <<< NEW: Import Hugging Face datasets library >>>
try:
    from datasets import load_dataset
    _HAS_HF_DATASETS = True
except ImportError:
    print("Warning: 'datasets' library not found. Loading datasets from Hugging Face Hub or local files (e.g., CSV) using --hf_dataset will be unavailable.")
    print("         Please install it: pip install datasets")
    _HAS_HF_DATASETS = False


# --- Optional Imports ---
try:
    from peft import get_peft_model, LoraConfig, TaskType, PeftModel
    _HAS_PEFT = True
except ImportError:
    print("Warning: 'peft' library not found. LoRA fine-tuning and merging will be unavailable.")
    _HAS_PEFT = False

# <<< MODIFIED: Removed keyboard import >>>

# --- Optimizer Selection (Handles CUDA, bitsandbytes, and CPU fallback) ---
_HAS_BNB = False # Default assumption
_HAS_AMP = False # Default assumption
_torch_amp_available = False # Track if torch.amp or torch.cuda.amp was found

# --- Try importing autocast and GradScaler first (unconditionally) ---
try:
    # Prefer torch.amp (newer PyTorch versions)
    from torch.amp import GradScaler, autocast
    print("INFO: torch.amp (autocast/GradScaler) found.")
    _torch_amp_available = True
except ImportError:
    try:
        # Fallback for older PyTorch versions (requires CUDA context, but import might work)
        from torch.cuda.amp import GradScaler, autocast
        print("INFO: torch.cuda.amp (autocast/GradScaler) found.")
        _torch_amp_available = True
    except ImportError:
        print("Warning: torch.amp and torch.cuda.amp not found.")
        _torch_amp_available = False
        # --- Define dummy autocast if import failed ---
        import contextlib
        @contextlib.contextmanager
        def autocast(device_type, enabled=True, dtype=None): # Match signature
            # print("Warning: Using dummy autocast context manager.") # Optional: uncomment for debugging
            yield
        # GradScaler is only needed if _HAS_AMP is True later, so no dummy needed now

# --- Now, determine optimizer and final AMP status based on CUDA ---
if torch.cuda.is_available():
    # Attempt to use bitsandbytes 8-bit AdamW if available
    try:
        import bitsandbytes as bnb
        ADAM_OPTIMIZER = bnb.optim.AdamW8bit
        _HAS_BNB = True
        print("INFO: CUDA detected and bitsandbytes found. Using bitsandbytes 8-bit AdamW.")
    except ImportError:
        print("Warning: bitsandbytes not found.")
        print("INFO: Falling back to standard torch.optim.AdamW optimizer.")
        ADAM_OPTIMIZER = torch.optim.AdamW
        _HAS_BNB = False

    # Check if the actual AMP components were successfully imported earlier
    if _torch_amp_available:
        _HAS_AMP = True
        print("INFO: AMP support is enabled (CUDA available).")
    else:
        _HAS_AMP = False
        print("Warning: AMP support is disabled (torch.amp/torch.cuda.amp import failed).")

else: # No CUDA detected
    print("Warning: CUDA not detected. Using CPU training.")
    print("INFO: Falling back to standard torch.optim.AdamW optimizer (bitsandbytes requires CUDA).")
    ADAM_OPTIMIZER = torch.optim.AdamW
    _HAS_BNB = False
    _HAS_AMP = False # AMP not usable on CPU
    if _torch_amp_available:
        # This case means torch.amp was importable but we are on CPU
        print("INFO: AMP components found but disabled (running on CPU).")
    # If _torch_amp_available is False, the warning about dummy autocast will show if used


# --- END Optimizer Selection ---


# --- C++ Kernel Import ---
try:
    import hierarchos_matmul
    _HAS_KERNEL = True
    print("Successfully imported C++ quantization kernel.")
    if hasattr(hierarchos_matmul, "VULKAN_SUPPORT") and hierarchos_matmul.VULKAN_SUPPORT:
        print("INFO: Vulkan support is enabled in the compiled kernel.")
        _HAS_VULKAN = True
    else:
        print("INFO: Vulkan support is disabled in the compiled kernel.")
        _HAS_VULKAN = False
except ImportError:
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("!!! WARNING: The compiled C++ kernel 'hierarchos_matmul' was not found.       !!!")
    print("!!!          Quantization and quantized inference will be unavailable.     !!!")
    print("!!!                                                                       !!!")
    print("!!! To enable these features, please run the appropriate setup script:    !!!")
    print("!!!  - On Windows:   Run setup.bat                                        !!!")
    print("!!!  - On Linux/macOS: Run bash setup.sh                                    !!!")
    print("!!!                                                                       !!!")
    print("!!! Make sure you have CMake and a C++ compiler installed.                 !!!")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    _HAS_KERNEL = False
    _HAS_VULKAN = False


# --- CONSTANTS ---
MODEL_WEIGHTS_NAME = "hierarchos.pt"
QUANTIZED_MODEL_WEIGHTS_NAME_TPL = "hierarchos-{qtype}.npz" # Template for the name


# --- HELPER CLASS: AttrDict ---
class AttrDict(dict):
    """A dictionary that allows for attribute-style access."""
    def __init__(self, *args, **kwargs):
        super(AttrDict, self).__init__(*args, **kwargs)
        self.__dict__ = self


# --- Device Utilities ---
def pick_device():
    """Picks the best available device for PyTorch training."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    # Commenting out MPS check for stability, can be re-enabled if needed
    # if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    #      return torch.device("mps")
    return torch.device("cpu")

def set_threads(n: int):
    try:
        torch.set_num_threads(n)
        os.environ['OMP_NUM_THREADS'] = str(n)
    except Exception as e:
        print(f"Warning: Could not set thread count. {e}")


# --- Dataset & Dataloader ---

# <<< START: New Iterable Dataset for Pre-Chunked JSONL Data >>>
class IterableChunkedJSONLDataset(IterableDataset):
    """
    An IterableDataset for loading pre-tokenized, chunked, masked, and padded
    data from a JSONL file line by line. Reduces RAM usage compared to loading
    the entire dataset into memory.

    Expects each line to be a JSON object containing 'input_ids', 'labels',
    and 'attention_mask' as lists of integers, all of the *same* pre-defined length.
    """
    def __init__(self, path: str, max_length: int):
        super().__init__()
        if not os.path.exists(path):
            raise FileNotFoundError(f"Dataset file not found: {path}")
        self.path = path
        self.max_length = max_length # Used for validation

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        worker_id = worker_info.id if worker_info is not None else 0
        num_workers = worker_info.num_workers if worker_info is not None else 1

        print(f"[Worker {worker_id}/{num_workers}] Opening dataset file: {self.path}")
        skipped_count = 0
        processed_count = 0

        try:
            with open(self.path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f):
                    # Distribute lines across workers
                    if line_num % num_workers != worker_id:
                        continue

                    line = line.strip()
                    if not line: continue

                    try:
                        obj = json.loads(line)
                        # --- Basic Validation ---
                        if not all(k in obj for k in ["input_ids", "labels", "attention_mask"]):
                            # Print less frequently in multi-worker scenarios to avoid spam
                            if skipped_count % (100 * num_workers) == 0:
                                print(f"[Worker {worker_id}] Warning: Skipping line ~{line_num+1}. Missing required keys.")
                            skipped_count += 1
                            continue
                        if not all(isinstance(obj[k], list) for k in ["input_ids", "labels", "attention_mask"]):
                            if skipped_count % (100 * num_workers) == 0:
                                print(f"[Worker {worker_id}] Warning: Skipping line ~{line_num+1}. Required keys are not lists.")
                            skipped_count += 1
                            continue

                        # --- Length Validation ---
                        seq_len = len(obj["input_ids"])
                        if seq_len != self.max_length:
                            if skipped_count % (100 * num_workers) == 0:
                                print(f"[Worker {worker_id}] Warning: Skipping line ~{line_num+1}. Expected length {self.max_length}, found {seq_len}.")
                            skipped_count += 1
                            continue
                        elif len(obj["labels"]) != seq_len or len(obj["attention_mask"]) != seq_len:
                            if skipped_count % (100 * num_workers) == 0:
                                print(f"[Worker {worker_id}] Warning: Skipping line ~{line_num+1}. Length mismatch between input_ids, labels, and attention_mask.")
                            skipped_count += 1
                            continue

                        # Convert lists to tensors just before yielding
                        processed_count += 1
                        yield {
                            "input_ids": torch.tensor(obj["input_ids"], dtype=torch.long),
                            "labels": torch.tensor(obj["labels"], dtype=torch.long),
                            "attention_mask": torch.tensor(obj["attention_mask"], dtype=torch.long)
                        }

                    except json.JSONDecodeError:
                        if skipped_count % (100 * num_workers) == 0:
                            print(f"[Worker {worker_id}] Warning: Skipping invalid JSON on line ~{line_num+1}: {line[:100]}...")
                        skipped_count += 1
                        continue
                    except Exception as e:
                        if skipped_count % (100 * num_workers) == 0:
                            print(f"[Worker {worker_id}] Warning: Error processing line ~{line_num+1}: {e}. Line: {line[:100]}...")
                        skipped_count += 1
                        continue
        except Exception as e:
            print(f"[Worker {worker_id}] ERROR during dataset iteration: {e}")
            traceback.print_exc() # Print full traceback if file reading fails etc.
            raise e # Re-raise the exception

        print(f"[Worker {worker_id}] Finished iterating. Processed: {processed_count}, Skipped: {skipped_count}")


def create_dataloader_for_chunked(path, max_length, batch_size, num_workers=0):
    """
    Creates a DataLoader specifically for the pre-chunked JSONL dataset using
    an IterableDataset to save RAM. Padding is assumed handled by the chunking script.
    """
    # Use the IterableDataset
    dataset = IterableChunkedJSONLDataset(path, max_length=max_length)

    def collate_fn_simple(batch):
        # Batch items are dictionaries with tensors of the same length
        if not batch: return None

        # Check if items are already dictionaries (expected from IterableDataset)
        if not isinstance(batch[0], dict):
            print(f"Warning: Unexpected item type in collate_fn_simple: {type(batch[0])}. Expected dict.")
            # Attempt to handle if it's a list/tuple, otherwise raise error
            if isinstance(batch[0], (list, tuple)) and len(batch[0]) == 3: # Assuming order: ids, labels, mask
                input_ids_batch = torch.stack([item[0] for item in batch])
                labels_batch = torch.stack([item[1] for item in batch])
                attention_mask_batch = torch.stack([item[2] for item in batch])
            else:
                raise TypeError(f"Collate function received unexpected data structure: {type(batch[0])}")
        else:
            input_ids_batch = torch.stack([item['input_ids'] for item in batch])
            labels_batch = torch.stack([item['labels'] for item in batch])
            attention_mask_batch = torch.stack([item['attention_mask'] for item in batch])

        return {
            "input_ids": input_ids_batch,
            "labels": labels_batch,
            "attention_mask": attention_mask_batch
        }

    pin_memory = torch.cuda.is_available() and num_workers > 0
    # persistent_workers is generally recommended with num_workers > 0
    # It avoids worker startup overhead for each epoch.
    persistent_workers = num_workers > 0

    # <<< MODIFIED: Removed shuffle=True, as it's not applicable here >>>
    # IterableDatasets handle shuffling differently (often requiring buffering)
    # or rely on the inherent order/distribution for large datasets.
    # Simple line-by-line iteration per worker is used here.
    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn_simple,
                      num_workers=num_workers, pin_memory=pin_memory,
                      persistent_workers=persistent_workers)
# <<< END: Iterable Dataset/Loader for Pre-Chunked JSONL Data >>>

# <<< START: Modified Map-Style Dataset/Loader for Consolidated PT Tensors >>>
class PTChunkedDataset(Dataset):
    """
    A map-style Dataset for loading pre-tokenized, chunked, masked, and padded
    data directly from individual chunk entries listed in a manifest.jsonl,
    where multiple chunks are consolidated into single .pt files.
    Reduces RAM usage during startup compared to loading all text.

    Expects a directory containing consolidated .pt files (each a list of dicts)
    and a manifest.jsonl. Each line in manifest.jsonl should contain
    'file_path' (relative) and 'index_in_file' (int).
    Each chunk dict within the .pt files should have 'input_ids', 'labels',
    and 'attention_mask' as torch.tensors of the *same* pre-defined length.
    Implements caching to reduce redundant file reads.
    """
    def __init__(self, directory_path: str, max_length: int):
        super().__init__()
        self.directory_path = directory_path
        self.max_length = max_length # For potential validation
        # <<< MODIFIED: Store (filepath, index) tuples >>>
        self.chunk_pointers = []
        # <<< MODIFIED: Add caching attributes >>>
        self.last_loaded_path = None
        self.last_loaded_data = None # This will hold the list loaded from a .pt file

        manifest_file = os.path.join(directory_path, "manifest.jsonl")
        if not os.path.exists(manifest_file):
            raise FileNotFoundError(f"Manifest file not found: {manifest_file}")

        print(f"Loading chunk pointers from manifest: {manifest_file}")
        try:
            with open(manifest_file, "r", encoding="utf-8") as f_manifest:
                for line_num, line in enumerate(f_manifest): # Added line_num for better warnings
                    line = line.strip()
                    if not line: continue
                    try:
                        entry = json.loads(line)
                        relative_path = entry.get("file_path")
                        # <<< MODIFIED: Get index_in_file >>>
                        index_in_file = entry.get("index_in_file")

                        # <<< MODIFIED: Validate entry >>>
                        if relative_path and isinstance(relative_path, str) and \
                           index_in_file is not None and isinstance(index_in_file, int):
                            full_path = os.path.join(self.directory_path, relative_path)
                            self.chunk_pointers.append((full_path, index_in_file))
                        else:
                            print(f"Warning: Manifest line ~{line_num+1} missing or invalid 'file_path' (str) or 'index_in_file' (int): {line}")
                    except json.JSONDecodeError:
                        print(f"Warning: Skipping invalid JSON in manifest line ~{line_num+1}: {line[:100]}...")
            if not self.chunk_pointers:
                raise ValueError(f"No valid chunk pointers found in manifest: {manifest_file}")
            print(f"Found {len(self.chunk_pointers)} total logical chunks.")
        except Exception as e:
            print(f"Error reading manifest file {manifest_file}: {e}")
            raise e

    def __len__(self):
        # <<< MODIFIED: Length is the total number of pointers >>>
        return len(self.chunk_pointers)

    def __getitem__(self, idx):
        # <<< MODIFIED: Retrieve path and index >>>
        try:
            chunk_path, index_in_file = self.chunk_pointers[idx]
        except IndexError:
            # This shouldn't happen with standard DataLoader usage but is a safeguard
            print(f"Error: Index {idx} out of bounds for chunk pointers (len: {len(self.chunk_pointers)}).")
            return None # Indicate failure

        try:
            # <<< MODIFIED: Implement Caching >>>
            if chunk_path == self.last_loaded_path:
                # Cache Hit: Use the already loaded list
                if self.last_loaded_data is None:
                    # Should not happen if last_loaded_path is set, but handle defensively
                    print(f"Warning: Cache inconsistency for file {chunk_path}. Reloading.")
                    self.last_loaded_data = torch.load(chunk_path, map_location='cpu')
                    if not isinstance(self.last_loaded_data, list):
                            raise TypeError(f"Loaded data from {chunk_path} is not a list.")
                # Retrieve the specific chunk dictionary from the cached list
                data = self.last_loaded_data[index_in_file]

            else:
                # Cache Miss: Load the new consolidated file
                # print(f"Cache miss. Loading file: {chunk_path}") # Optional: for debugging
                loaded_list = torch.load(chunk_path, map_location='cpu') # Load to CPU initially
                if not isinstance(loaded_list, list):
                    raise TypeError(f"Loaded data from {chunk_path} is not a list.")

                # Update cache
                self.last_loaded_path = chunk_path
                self.last_loaded_data = loaded_list

                # Retrieve the specific chunk dictionary
                data = self.last_loaded_data[index_in_file]

            # --- Validation (Optional but recommended) ---
            if not isinstance(data, dict):
                raise TypeError(f"Chunk at index {index_in_file} in {chunk_path} is not a dictionary.")
            if not all(k in data for k in ["input_ids", "labels", "attention_mask"]):
                print(f"Warning: Chunk dict at index {index_in_file} in {chunk_path} missing required keys. Skipping.")
                return None # Indicate failure
            if not all(isinstance(data[k], torch.Tensor) for k in ["input_ids", "labels", "attention_mask"]):
                print(f"Warning: Data in chunk dict at index {index_in_file} in {chunk_path} are not tensors. Skipping.")
                return None
            if data["input_ids"].shape[0] != self.max_length:
                print(f"Warning: Chunk tensor 'input_ids' at index {index_in_file} in {chunk_path} has unexpected length {data['input_ids'].shape[0]}. Expected {self.max_length}. Skipping.")
                return None
            # --- End Validation ---

            return data

        except FileNotFoundError:
             print(f"Error: Consolidated chunk file not found: {chunk_path}")
             self.last_loaded_path = None # Invalidate cache if file not found
             self.last_loaded_data = None
             return None
        except IndexError:
             print(f"Error: index_in_file {index_in_file} out of bounds for loaded list from {chunk_path} (len: {len(self.last_loaded_data) if self.last_loaded_data else 'N/A'}). Check manifest/chunking script.")
             # Consider invalidating cache here too if the file structure seems wrong
             # self.last_loaded_path = None
             # self.last_loaded_data = None
             return None
        except TypeError as e:
             print(f"Error: Type error processing chunk at index {index_in_file} in {chunk_path}: {e}")
             return None
        except Exception as e:
             # Catch other potential errors during loading or processing
             print(f"Error loading or processing chunk from {chunk_path} at index {index_in_file}: {e}")
             # Optionally invalidate cache on unexpected errors
             # self.last_loaded_path = None
             # self.last_loaded_data = None
             return None


def create_dataloader_pt_chunked(directory_path, max_length, batch_size, num_workers=0):
    """
    Creates a DataLoader for the pre-chunked consolidated .pt dataset using PTChunkedDataset.
    Handles shuffling and batching of pre-loaded tensors. Caching is handled within the Dataset.
    """
    dataset = PTChunkedDataset(directory_path, max_length=max_length) # Uses the MODIFIED dataset

    def collate_fn_pt(batch):
        # Filter out None items potentially returned by dataset __getitem__ on error
        batch = [item for item in batch if item is not None]
        if not batch: return None # Return None if batch becomes empty after filtering

        # Items are dictionaries with tensors of the same length
        try:
            input_ids_batch = torch.stack([item['input_ids'] for item in batch])
            labels_batch = torch.stack([item['labels'] for item in batch])
            attention_mask_batch = torch.stack([item['attention_mask'] for item in batch])
        except Exception as e:
            print(f"Error during collate_fn_pt: {e}. One of the items might be malformed.")
            # Decide how to handle this - skip batch or raise error?
            # Returning None might be safer if errors are expected, but hides issues.
            # Raising the error stops training but makes the problem explicit.
            # Let's return None for now to avoid crashing training on rare errors.
            return None


        return {
            "input_ids": input_ids_batch,
            "labels": labels_batch,
            "attention_mask": attention_mask_batch
        }

    pin_memory = torch.cuda.is_available() and num_workers > 0
    persistent_workers = num_workers > 0

    # Map-style dataset allows shuffling
    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn_pt, shuffle=True,
                      num_workers=num_workers, pin_memory=pin_memory,
                      persistent_workers=persistent_workers)

# <<< END: Modified Map-Style Dataset/Loader for Consolidated PT Tensors >>>


# <<< START: Utility function for processing text based on args (shared by JSONL and HF dataset) >>>
def process_text_sample(tokenizer, text_dict: dict, max_length: int, kayla_mode: bool = False,
                         text_column: Optional[str] = None,
                         prompt_column: Optional[str] = None, completion_column: Optional[str] = None):
    """
    Processes a dictionary containing text into tokenized input_ids and labels
    based on the specified mode (Kayla, instruction tuning, or text completion).
    Returns a dictionary {"input_ids": tensor, "labels": tensor} or None on error.
    """
    input_ids = []
    labels = []

    try:
        if text_column:
            # --- Text Completion Mode (e.g., pre-training) ---
            text = text_dict.get(text_column, "")
            if not isinstance(text, str): text = str(text) # Attempt conversion
            if not text: return None # Skip empty text

            input_ids = tokenizer.encode(text, add_special_tokens=True) + [tokenizer.eos_token_id]
            labels = list(input_ids) # Predict every token

        elif prompt_column and completion_column:
            # --- Instruction Tuning Mode (Standard or Kayla) ---
            if kayla_mode:
                # --- Kayla Format ---
                instruction = text_dict.get(prompt_column, "") # Use prompt_column for instruction
                completion = text_dict.get(completion_column, "") # Use completion_column for output
                # Assumes 'feelings' and 'thought-process' are also present in text_dict if needed
                feelings = text_dict.get('feelings', '')
                thought = text_dict.get('thought-process', '')

                if not isinstance(instruction, str): instruction = str(instruction)
                if not isinstance(completion, str): completion = str(completion)
                if not isinstance(feelings, str): feelings = str(feelings)
                if not isinstance(thought, str): thought = str(thought)


                instruction_text = f"### Instruction:\n{instruction}\n\n"
                feelings_text = f"### Feelings:\n{feelings}\n\n" if feelings else ""
                prompt_context_text = instruction_text + feelings_text
                thought_text = f"### Thought Process:\n{thought}\n\n"
                output_text = f"### Response:\n{completion}"

                prompt_context_tokens = tokenizer.encode(prompt_context_text, add_special_tokens=True)
                thought_tokens = tokenizer.encode(thought_text, add_special_tokens=False)
                output_tokens = tokenizer.encode(output_text, add_special_tokens=False)

                input_ids = prompt_context_tokens + thought_tokens + output_tokens + [tokenizer.eos_token_id]
                labels = ([-100] * len(prompt_context_tokens)) + thought_tokens + output_tokens + [tokenizer.eos_token_id]

            else:
                # --- Standard Format (Modified for Alpaca) ---
                instruction_text = text_dict.get(prompt_column, "")
                # Check for an 'input' column, which is common in Alpaca
                input_text = text_dict.get('input', "") 
                completion_text = text_dict.get(completion_column, "")

                if not isinstance(instruction_text, str): instruction_text = str(instruction_text)
                if not isinstance(input_text, str): input_text = str(input_text)
                if not isinstance(completion_text, str): completion_text = str(completion_text)

                # Combine instruction and input if input exists
                if input_text:
                    prompt_formatted = f"### Instruction:\n{instruction_text}\n\n### Input:\n{input_text}\n\n### Response:\n"
                else:
                    prompt_formatted = f"### Instruction:\n{instruction_text}\n\n### Response:\n"

                prompt_tokens = tokenizer.encode(prompt_formatted, add_special_tokens=True)
                completion_tokens = tokenizer.encode(completion_text, add_special_tokens=False)

                input_ids = prompt_tokens + completion_tokens + [tokenizer.eos_token_id]
                labels = ([-100] * len(prompt_tokens)) + completion_tokens + [tokenizer.eos_token_id]

        else:
            # Invalid configuration or missing required columns
            # Try a default 'text' column if nothing else specified
            text = text_dict.get('text', "")
            if isinstance(text, str) and text:
                print("Warning: No text/prompt/completion columns specified. Falling back to 'text' column for text completion.")
                input_ids = tokenizer.encode(text, add_special_tokens=True) + [tokenizer.eos_token_id]
                labels = list(input_ids)
            else:
                 print(f"Warning: Skipping data entry due to missing or invalid text columns: {list(text_dict.keys())}")
                 return None # Skip this sample

        # --- Truncation ---
        if len(input_ids) > max_length:
            input_ids = input_ids[:max_length-1] + [tokenizer.eos_token_id]
            labels = labels[:max_length-1] + [tokenizer.eos_token_id]

        # Padding is handled by the collate function
        return {"input_ids": torch.tensor(input_ids, dtype=torch.long), "labels": torch.tensor(labels, dtype=torch.long)}

    except Exception as e:
        obj_repr = str(text_dict)
        print(f"Warning: Skipping invalid data entry: {obj_repr[:150] + ('...' if len(obj_repr) > 150 else '')}. Error: {e}")
        traceback.print_exc(limit=1)
        return None

# <<< END: Utility function for processing text >>>


# <<< START: Original Dataset/Loader (Renamed) - Modified to use process_text_sample >>>
class OriginalJSONLDataset(Dataset):
    """
    Handles both .jsonl (one JSON object per line) and .json (a list of objects) files.
    Also supports standard and "Kayla" instruction formats using standard keys like
    'instruction', 'output', 'Instruction', 'thought-process', 'feelings'.
    Tokenizes data on the fly. Loads everything into RAM.
    """
    def __init__(self, path: str, tokenizer, max_length: int, kayla_mode: bool = False):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []
        self.kayla_mode = kayla_mode
        self._load(path)

    def _load(self, path):
        if not os.path.exists(path):
            raise FileNotFoundError(f"Dataset file not found: {path}")

        print(f"Loading and tokenizing dataset from {path}...")
        if self.kayla_mode:
            print("INFO: Kayla-style instruction tuning is ENABLED.")

        skipped_count = 0
        line_num = 0
        with open(path, "r", encoding="utf-8") as f:
            # Try loading as a single JSON list first
            try:
                f.seek(0) # Ensure reading from start
                data = json.load(f)
                if isinstance(data, list):
                    print("Detected JSON file (list of objects). Processing...")
                    for obj in tqdm(data, desc="Tokenizing samples"):
                        # <<< MODIFIED: Use shared processing function >>>
                        processed = process_text_sample(
                            self.tokenizer, obj, self.max_length, self.kayla_mode,
                            prompt_column='instruction', # Default keys for JSONL
                            completion_column='output'
                        )
                        if processed:
                            self.samples.append(processed)
                        else:
                            skipped_count += 1
                    if skipped_count > 0: print(f"Skipped {skipped_count} invalid samples during JSON loading.")
                    return # Successfully loaded JSON list
                else:
                    print("Warning: JSON file does not contain a list. Attempting JSONL parsing.")
            except json.JSONDecodeError:
                # This is expected for a JSONL file, so pass through
                pass
            except Exception as e: # Catch other potential errors during JSON load
                print(f"Warning: Error loading as JSON list: {e}. Attempting JSONL parsing.")

            # If not a JSON list or loading failed, try JSONL
            print("Attempting JSONL file (one object per line). Processing...")
            f.seek(0) # Reset file pointer
            skipped_count = 0 # Reset skipped count for JSONL
            for line in tqdm(f, desc="Tokenizing samples (JSONL)"):
                line_num += 1
                line = line.strip()
                if not line: continue
                try:
                    obj = json.loads(line)
                    # <<< MODIFIED: Use shared processing function >>>
                    processed = process_text_sample(
                        self.tokenizer, obj, self.max_length, self.kayla_mode,
                        prompt_column='instruction', # Default keys for JSONL
                        completion_column='output'
                    )
                    if processed:
                        self.samples.append(processed)
                    else:
                        skipped_count += 1
                except json.JSONDecodeError:
                    # Reduce verbosity
                    if skipped_count % 1000 == 0:
                        print(f"\nWarning: Skipping invalid JSON on line ~{line_num}: {line[:100]}...")
                    skipped_count += 1
                    continue
                except Exception as e:
                    if skipped_count % 1000 == 0:
                        print(f"\nWarning: Error processing line ~{line_num}: {e}. Line: {line[:100]}...")
                    skipped_count += 1
                    continue

        if skipped_count > 0: print(f"Skipped {skipped_count} invalid samples during JSONL loading.")


    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

# <<< NEW: Hugging Face Map-Style Dataset Class >>>
class HuggingFaceMapStyleDataset(Dataset):
    """
    Wraps a Hugging Face dataset (already loaded) for use with hierarchos.
    Handles tokenization and formatting on the fly based on specified columns.
    """
    def __init__(self, hf_dataset, tokenizer, max_length: int, kayla_mode: bool = False,
                 text_column: Optional[str] = None,
                 prompt_column: Optional[str] = None, completion_column: Optional[str] = None):
        super().__init__()
        self.hf_dataset = hf_dataset
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.kayla_mode = kayla_mode
        self.text_column = text_column
        self.prompt_column = prompt_column
        self.completion_column = completion_column

        # --- Validate column existence ---
        ds_columns = hf_dataset.column_names
        if self.text_column and self.text_column not in ds_columns:
            raise ValueError(f"Specified text_column '{self.text_column}' not found in dataset columns: {ds_columns}")
        if self.prompt_column and self.prompt_column not in ds_columns:
            raise ValueError(f"Specified prompt_column '{self.prompt_column}' not found in dataset columns: {ds_columns}")
        if self.completion_column and self.completion_column not in ds_columns:
             raise ValueError(f"Specified completion_column '{self.completion_column}' not found in dataset columns: {ds_columns}")
        if not self.text_column and not (self.prompt_column and self.completion_column):
            if 'text' in ds_columns:
                print(f"Warning: No specific columns provided, defaulting to using 'text' column for text completion.")
                self.text_column = 'text' # Default fallback
            else:
                raise ValueError(f"Must specify either --text_column OR (--prompt_column AND --completion_column). Available columns: {ds_columns}")


    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        # Fetch the raw data entry from the Hugging Face dataset
        raw_item = self.hf_dataset[idx]
        if not isinstance(raw_item, dict):
            print(f"Warning: Unexpected data type from HF dataset at index {idx}: {type(raw_item)}. Skipping.")
            return None # Should return None to be filtered by collate_fn

        # Process the text using the shared utility function
        processed = process_text_sample(
            self.tokenizer, raw_item, self.max_length, self.kayla_mode,
            self.text_column, self.prompt_column, self.completion_column
        )
        return processed # process_text_sample returns None on error


# <<< START: NEW TOP-LEVEL COLLATE FUNCTION >>>
def _collate_fn_dynamic_padding(batch, pad_token_id: int):
    """
    Top-level collate function for map-style datasets, handling dynamic padding.
    MUST be top-level for multiprocessing (num_workers > 0) to work on Windows/spawn.
    """
    # Filter out None items potentially returned by dataset __getitem__ if processing failed
    batch = [item for item in batch if item is not None]
    if not batch: return None # Return None if batch becomes empty

    # Find max length *in this batch* for dynamic padding
    max_len_batch = max(len(item['input_ids']) for item in batch)

    input_ids_batch = torch.full((len(batch), max_len_batch), pad_token_id, dtype=torch.long)
    labels_batch = torch.full((len(batch), max_len_batch), -100, dtype=torch.long) # Use -100 for padding labels
    attention_mask_batch = torch.zeros((len(batch), max_len_batch), dtype=torch.long)

    for i, item in enumerate(batch):
        seq_len = len(item['input_ids'])
        # Copy sequence data
        input_ids_batch[i, :seq_len] = item['input_ids']
        labels_batch[i, :seq_len] = item['labels']
        attention_mask_batch[i, :seq_len] = 1 # Set attention mask to 1 for non-pad tokens

    return {
        "input_ids": input_ids_batch,
        "labels": labels_batch,
        "attention_mask": attention_mask_batch
    }
# <<< END: NEW TOP-LEVEL COLLATE FUNCTION >>>


# <<< Modified original dataloader creator function to handle ANY map-style dataset >>>
def create_map_style_dataloader(
    dataset: Dataset, # Accepts OriginalJSONLDataset or HuggingFaceMapStyleDataset
    batch_size: int,
    pad_token_id: int,
    num_workers: int = 0,
    shuffle: bool = True
):
    """
    Creates a DataLoader for map-style datasets (like OriginalJSONLDataset or HuggingFaceMapStyleDataset),
    handling dynamic padding.
    """
    if len(dataset) == 0:
        raise ValueError("Dataset provided to create_map_style_dataloader is empty or invalid.")

    # <<< FIX: Use functools.partial to create a collate_fn instance >>>
    # This passes the pad_token_id to our new top-level function.
    # This is necessary for multiprocessing (num_workers > 0) to work.
    collate_fn_with_padding = functools.partial(
        _collate_fn_dynamic_padding,
        pad_token_id=pad_token_id
    )

    pin_memory = torch.cuda.is_available() and num_workers > 0
    persistent_workers = num_workers > 0
    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn_with_padding, shuffle=shuffle,
                        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=persistent_workers)

# <<< END: Dataloader modifications >>>


# --- Quantization & Model Serialization ---
# ... (Quantization code remains unchanged - assuming hierarchos_matmul is available) ...
def get_q_block_size(qtype: str) -> int:
    """Returns the block size for a given quantization type."""
    if qtype in ["INT4", "Q4_0", "Q8_0"]:
        return 32
    elif qtype == "Q2_K":
        return 256
    else:
        raise ValueError(f"Unknown quantization type: {qtype}")

def export_and_quantize_model(output_dir: str, model: nn.Module, tokenizer, qtype: str):
    """Quantizes and exports the model to a directory containing the .npz and tokenizer."""
    if not _HAS_KERNEL:
        print("ERROR: C++ kernel is required for quantization. Aborting.")
        return

    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, QUANTIZED_MODEL_WEIGHTS_NAME_TPL.format(qtype=qtype))

    print(f"Quantizing and exporting model to {output_path} with {qtype} weights...")
    state_dict = model.state_dict()
    quantized_tensors = {}

    # Save the model config directly into the npz file
    config_to_save = dict(model.config)
    quantized_tensors['_config'] = np.array(config_to_save, dtype=object) # Use dtype=object for dict

    Q_BLOCK_SIZE = get_q_block_size(qtype)

    for name, tensor in tqdm(state_dict.items(), desc="Quantizing Tensors"):
        # Include a check for 1D tensors (like biases and layernorm weights) and exclude LTM metadata
        if tensor.ndim == 2 and "emb" not in name and "ltm" not in name and "timestamps" not in name and "sources" not in name:
            float_tensor_np = tensor.cpu().float().numpy()
            M, K = float_tensor_np.shape

            pad_cols = 0
            if K % Q_BLOCK_SIZE != 0:
                pad_cols = Q_BLOCK_SIZE - (K % Q_BLOCK_SIZE)
                float_tensor_np = np.pad(float_tensor_np, ((0, 0), (0, pad_cols)), 'constant')

            quantized_data = hierarchos_matmul.quantize(float_tensor_np, qtype)
            quantized_tensors[name] = {
                "quantized": quantized_data,
                "qtype": qtype,
                "original_shape": [M, K],
            }
        # Keep 1D tensors and explicitly excluded layers as raw numpy arrays
        elif tensor.ndim >= 1 and ("emb" in name or "norm" in name or "bias" in name or "persistent" in name or "ltm." in name):
            # Exclude internal LTM buffers explicitly
            if name != 'ltm._mom_vals' and name != 'ltm.ltm_deltas':
                quantized_tensors[name] = {"raw": tensor.cpu().numpy()}
            else:
                print(f"Skipping internal LTM buffer '{name}' from quantization file.")
        else:
            print(f"Skipping tensor '{name}' with shape {tensor.shape} from quantization.")


    np.savez_compressed(output_path, **quantized_tensors)
    print(f"Model weights successfully exported to {output_path}")

    # Also save the tokenizer to make the directory self-contained
    try:
        tokenizer.save_pretrained(output_dir)
        print(f"Tokenizer files saved to {output_dir}")
    except Exception as e:
        print(f"Warning: Failed to save tokenizer files to {output_dir}. Error: {e}")

class QuantizedLinear:
    """A wrapper for a quantized linear layer that uses the C++ kernel for inference."""
    def __init__(self, name: str, q_data: dict):
        self.name = name
        weight_data_key = f'{name}.weight'
        bias_data_key = f'{name}.bias'

        if weight_data_key not in q_data:
            raise KeyError(f"Weight data '{weight_data_key}' not found in quantized file.")

        weight_meta = q_data[weight_data_key].item() # .item() needed for numpy object arrays
        if 'quantized' not in weight_meta:
            raise ValueError(f"Weight '{weight_data_key}' is not quantized (missing 'quantized' key).")

        self.quantized_w = weight_meta['quantized']
        self.qtype = str(weight_meta['qtype'])
        self.original_shape = weight_meta['original_shape']
        self.M, self.K = self.original_shape

        if bias_data_key in q_data:
            bias_meta = q_data[bias_data_key].item() # .item() needed
            if 'raw' not in bias_meta:
                raise ValueError(f"Bias '{bias_data_key}' is missing 'raw' data.")
            self.bias = bias_meta['raw']
        else:
            self.bias = None

    def __call__(self, x: torch.Tensor, device: str = "cpu") -> torch.Tensor:
        if not _HAS_KERNEL: raise ImportError("C++ kernel required for quantized matmul")

        x_np = x.cpu().float().numpy()
        # Ensure x_np is 2D for matmul
        original_ndim = x_np.ndim
        original_shape = x_np.shape
        if original_ndim == 1:
            x_np = x_np.reshape(1, -1)
        elif original_ndim > 2:
            # Flatten leading dimensions if any (e.g., batch, sequence length)
            x_np = x_np.reshape(-1, x_np.shape[-1])

        # Ensure input K matches the quantized weight K (which is the original K + padding)
        # The C++ kernel expects the input K to match the *padded* K of the weight.
        padded_k = self.K
        if self.K % get_q_block_size(self.qtype) != 0:
            padded_k += get_q_block_size(self.qtype) - (self.K % get_q_block_size(self.qtype))

        if x_np.shape[-1] != padded_k:
            # Pad input if needed to match the kernel's expectation
            pad_k = padded_k - x_np.shape[-1]
            if pad_k > 0:
                x_np = np.pad(x_np, ((0, 0), (0, pad_k)), 'constant')
            elif pad_k < 0: # Input is larger than expected padded K? Should not happen.
                print(f"Warning: Input dimension ({x_np.shape[-1]}) > Expected padded K ({padded_k}) for layer {self.name}. Truncating input.")
                x_np = x_np[..., :padded_k]


        y_np = hierarchos_matmul.matmul_quantized(x_np, self.quantized_w, self.M, self.qtype, device)

        # Output shape should match original input dimensions + output features M
        if original_ndim > 2:
            output_shape = list(original_shape[:-1]) + [self.M]
            y_np = y_np.reshape(output_shape)
        elif original_ndim == 1:
            y_np = y_np.reshape(-1) # Reshape back to 1D


        if y_np.shape[-1] != self.M:
            # This should ideally not happen if matmul_quantized handles padding correctly,
            # but keep as a safeguard. Truncate output to expected feature dim M.
            y_np = y_np[..., :self.M]

        if self.bias is not None: y_np += self.bias
        return torch.from_numpy(y_np)

class QuantizedRWKVCell:
    def __init__(self, n_embd, name_prefix, q_data):
        self.n_embd = n_embd
        self.key = QuantizedLinear(f'{name_prefix}.key', q_data)
        self.value = QuantizedLinear(f'{name_prefix}.value', q_data)
        self.receptance = QuantizedLinear(f'{name_prefix}.receptance', q_data)
        self.output = QuantizedLinear(f'{name_prefix}.output', q_data)
        self.key_cm = QuantizedLinear(f'{name_prefix}.key_cm', q_data)
        self.receptance_cm = QuantizedLinear(f'{name_prefix}.receptance_cm', q_data)
        self.value_cm = QuantizedLinear(f'{name_prefix}.value_cm', q_data)

        def load_raw(name):
            return torch.from_numpy(q_data[f'{name_prefix}.{name}'].item()['raw'])

        self.time_decay = load_raw('time_decay')
        self.time_first = load_raw('time_first')
        self.time_mix_k = load_raw('time_mix_k')
        self.time_mix_v = load_raw('time_mix_v')
        self.time_mix_r = load_raw('time_mix_r')
        self.time_mix_k_cm = load_raw('time_mix_k_cm')
        self.time_mix_r_cm = load_raw('time_mix_r_cm')

    def __call__(self, x, state, device="cpu"):
        for p in [self.time_decay, self.time_first, self.time_mix_k, self.time_mix_v,
                  self.time_mix_r, self.time_mix_k_cm, self.time_mix_r_cm]:
            if p.device.type != device:
                p = p.to(device)

        sx, aa, bb, pp, sx_cm = state.unbind(dim=2)

        # Time mixing
        xk = x * self.time_mix_k + sx * (1 - self.time_mix_k)
        xv = x * self.time_mix_v + sx * (1 - self.time_mix_v)
        xr = x * self.time_mix_r + sx * (1 - self.time_mix_r)

        r = torch.sigmoid(self.receptance(xr, device))
        k = self.key(xk, device)
        v = self.value(xv, device)

        ww = self.time_first + k
        p = torch.maximum(pp, ww)
        e1 = torch.exp(pp - p)
        e2 = torch.exp(ww - p)
        wkv = (e1 * aa + e2 * v) / (e1 * bb + e2 + 1e-8)
        x = x + self.output(r * wkv, device)

        ww = pp + self.time_decay
        p = torch.maximum(ww, k)
        e1 = torch.exp(ww - p)
        e2 = torch.exp(k - p)
        aa = e1 * aa + e2 * v
        bb = e1 * bb + e2
        pp = p

        # Channel mixing
        xk = x * self.time_mix_k_cm + sx_cm * (1 - self.time_mix_k_cm)
        xr = x * self.time_mix_r_cm + sx_cm * (1 - self.time_mix_r_cm)
        r = torch.sigmoid(self.receptance_cm(xr, device))
        k = torch.square(torch.relu(self.key_cm(xk, device)))
        x = x + r * self.value_cm(k, device)

        new_state = torch.stack([x, aa, bb, pp, x], dim=2)
        return x, new_state

class RWKVCell(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.n_embd = n_embd

        # --- CRITICAL FIX: Time Decay Initialization ---
        # Cannot be 0. If 0, exp(0)=1, meaning history never decays and state explodes.
        # We initialize from -6 to -1 across channels.
        # This creates a range of retention windows (short-term vs long-term).
        decay_speed = torch.arange(0, n_embd) / n_embd
        self.time_decay = nn.Parameter(-5 + 4 * decay_speed) # Range: [-5, -1]

        # --- Time First (Bonus) ---
        # This acts as a bias for the current token's key against the history.
        self.time_first = nn.Parameter(torch.ones(n_embd) * 0.5)
        
        # --- CRITICAL FIX: Time Mixing Initialization ---
        # Previous zero-init meant xk = sx (100% history, 0% input).
        # We use a curve so some channels focus on input (1.0) and others on history (0.0).
        curve = torch.arange(0, n_embd) / n_embd
        curve = torch.pow(curve, 0.5) # Simple power curve

        # Init as (1, 1, n_embd) for broadcasting
        self.time_mix_k = nn.Parameter(curve.view(1, 1, n_embd))
        self.time_mix_v = nn.Parameter(curve.view(1, 1, n_embd) + 0.1 * torch.randn(1, 1, n_embd)) # Slight variance for V
        self.time_mix_r = nn.Parameter(0.5 * curve.view(1, 1, n_embd)) # Receptance usually mixes more history

        self.key = nn.Linear(n_embd, n_embd, bias=False)
        self.value = nn.Linear(n_embd, n_embd, bias=False)
        self.receptance = nn.Linear(n_embd, n_embd, bias=False)
        self.output = nn.Linear(n_embd, n_embd, bias=False)

        # --- Channel Mixing ---
        # Channel mixing usually benefits from starting with a small bias towards history 
        # but not purely dead (0). We init small positive values.
        self.time_mix_k_cm = nn.Parameter(torch.ones(1, 1, n_embd) * 0.05)
        self.time_mix_r_cm = nn.Parameter(torch.ones(1, 1, n_embd) * 0.05)

        self.key_cm = nn.Linear(n_embd, n_embd * 4, bias=False)
        self.receptance_cm = nn.Linear(n_embd, n_embd, bias=False)
        self.value_cm = nn.Linear(n_embd * 4, n_embd, bias=False)

    def forward(self, x, state):
        # --- FIX: Handle torch.compile artifacts (fake extra batch dim) ---
        # We verify dimensions to ensure we are working with [Batch, Hidden]
        if x.dim() == 3 and x.shape[0] == 1:
            x = x.squeeze(0)
        if state.dim() == 4 and state.shape[0] == 1:
            state = state.squeeze(0)
        # ------------------------------------------------------------------

        # Flatten mixing parameters to 1D to prevent accidental 3D broadcasting
        # This ensures input [B, H] * param [H] -> [B, H], not [1, B, H]
        tm_k = self.time_mix_k.view(-1)
        tm_v = self.time_mix_v.view(-1)
        tm_r = self.time_mix_r.view(-1)
        tm_k_cm = self.time_mix_k_cm.view(-1)
        tm_r_cm = self.time_mix_r_cm.view(-1)

        # Use dim=-1 (last dimension) to be robust against leading batch dims
        sx, aa, bb, pp, sx_cm = state.unbind(dim=-1)

        # --- Time mixing ---
        xk = x * tm_k + sx * (1 - tm_k)
        xv = x * tm_v + sx * (1 - tm_v)
        xr = x * tm_r + sx * (1 - tm_r)

        r = torch.sigmoid(self.receptance(xr))
        k = self.key(xk)
        v = self.value(xv)

        # WKV Computation (RWKV-4 style)
        ww = self.time_first + k
        p = torch.maximum(pp, ww)
        e1 = torch.exp(pp - p)
        e2 = torch.exp(ww - p)
        wkv = (e1 * aa + e2 * v) / (e1 * bb + e2 + 1e-8)
        x = x + self.output(r * wkv)

        # Update state
        ww = pp + self.time_decay
        p = torch.maximum(ww, k)
        e1 = torch.exp(ww - p)
        e2 = torch.exp(k - p)
        aa = e1 * aa + e2 * v
        bb = e1 * bb + e2
        pp = p

        # --- Channel mixing ---
        xk = x * tm_k_cm + sx_cm * (1 - tm_k_cm)
        xr = x * tm_r_cm + sx_cm * (1 - tm_r_cm)
        r = torch.sigmoid(self.receptance_cm(xr))
        k = torch.square(torch.relu(self.key_cm(xk)))
        x = x + r * self.value_cm(k)

        # NEW STATE: Stack on last dimension (dim=-1)
        new_state = torch.stack([x, aa, bb, pp, x], dim=-1)

        return x, new_state
class LTMModule(nn.Module):
    """Titans Long-Term Memory Module. Capable of test-time updates."""
    # SOURCE_ID definitions
    SRC_UNKNOWN = 0
    SRC_USER_INTERACTION = 1
    SRC_TRAINING_DATA = 2

    def __init__(self, n_slots=1024, key_dim=64, val_dim=64, lr=1e-3, momentum=0.9, wd=1e-4):
        super().__init__()
        # Keys must remain random so they are distinct and addressable by attention queries
        self.keys = nn.Parameter(torch.randn(n_slots, key_dim) * 0.02)
        
        # FIX 1: Zero Initialization for Values
        # This prevents random noise injection before the model learns to write data.
        self.vals = nn.Parameter(torch.zeros(n_slots, val_dim))
        
        self.register_buffer("_mom_vals", torch.zeros_like(self.vals.data))
        self.lr, self.momentum, self.weight_decay = lr, momentum, wd

        # Buffers are not parameters; they are part of the model's state
        self.register_buffer("timestamps", torch.zeros(n_slots, dtype=torch.float32))
        self.register_buffer("sources", torch.full((n_slots,), self.SRC_UNKNOWN, dtype=torch.long))

        # Buffer for accumulating deltas if not updating in-place
        self.register_buffer("ltm_deltas", torch.zeros_like(self.vals.data))
        self.accumulate_deltas = False


    def retrieve_topk(self, queries: torch.Tensor, topk: int = 4, min_timestamp: float = 0.0, source_filter: Optional[int] = None) -> Tuple[torch.Tensor, torch.LongTensor]:
        """
        Retrieves the top-k most similar values from memory, with optional filtering
        by timestamp and source.
        """
        sim = queries @ self.keys.t()

        # Apply filters by creating a mask of valid slots
        if min_timestamp > 0.0 or source_filter is not None:
            with torch.no_grad():
                # Start with all slots being valid
                valid_mask = torch.ones(self.keys.size(0), dtype=torch.bool, device=self.keys.device)

                if min_timestamp > 0.0:
                    # Filter out memories that are older than the specified timestamp
                    valid_mask &= (self.timestamps >= min_timestamp)

                if source_filter is not None:
                    # Filter for a specific source
                    valid_mask &= (self.sources == source_filter)

                # Set the similarity score of invalid slots to negative infinity
                # so they are never chosen by topk. Handle potential NaNs/Infs in sim first.
                sim = torch.nan_to_num(sim, nan=-torch.inf, posinf=torch.finfo(sim.dtype).max, neginf=-torch.inf)
                sim[:, ~valid_mask] = -torch.inf


        # Ensure topk is not greater than the number of valid slots
        num_valid_slots_per_query = sim.isfinite().sum(dim=-1) # Shape: [Batch] or [Batch, SeqLen] etc.
        num_valid_slots = num_valid_slots_per_query.min().item() # Minimum valid slots across all queries
        effective_topk = min(topk, int(num_valid_slots))

        if effective_topk <= 0:
            # Handle case where no slots match the filter for at least one query
            # print("Warning: No LTM slots matched the current filter criteria for at least one query.")
            # Return tensors filled with zeros/dummy values matching expected shape
            query_shape = list(queries.shape)
            # Shape: [..., TopK, ValDim] (handles arbitrary leading dims)
            vals_shape = query_shape[:-1] + [topk, self.vals.shape[-1]]
            # Shape: [..., TopK]
            idx_shape = query_shape[:-1] + [topk]
            return torch.zeros(vals_shape, device=queries.device, dtype=self.vals.dtype), \
                   torch.full(idx_shape, -1, device=queries.device, dtype=torch.long) # Use -1 for invalid index


        _, idx = torch.topk(sim, k=effective_topk, dim=-1) # Shape: [..., effective_topk]

        # Pad results if effective_topk < topk
        if effective_topk < topk:
            pad_size = topk - effective_topk
            # Pad indices with -1
            idx_pad_shape = list(idx.shape[:-1]) + [pad_size]
            idx_pad = torch.full(idx_pad_shape, -1, device=idx.device, dtype=idx.dtype)
            idx = torch.cat([idx, idx_pad], dim=-1) # Shape: [..., topk]

            # Pad retrieved values with zeros
            # Need to handle potential -1 indices introduced by filtering before indexing self.vals
            valid_idx_mask = idx[..., :effective_topk] >= 0
            vals_retrieved = torch.zeros(list(idx.shape[:-1]) + [effective_topk, self.vals.shape[-1]], device=self.vals.device, dtype=self.vals.dtype)
            # Only index self.vals where the mask is true
            if valid_idx_mask.any():
                actual_indices = idx[..., :effective_topk][valid_idx_mask]
                vals_retrieved[valid_idx_mask] = self.vals[actual_indices]

            vals_pad_shape = list(vals_retrieved.shape[:-2]) + [pad_size, vals_retrieved.shape[-1]]
            vals_pad = torch.zeros(vals_pad_shape, device=vals_retrieved.device, dtype=vals_retrieved.dtype)
            vals_ret = torch.cat([vals_retrieved, vals_pad], dim=-2) # Concatenate along the topk dimension
            return vals_ret, idx # Shape: [..., topk, ValDim], [..., topk]
        else:
            # If effective_topk == topk, we might still have filtered out slots, resulting in -inf
            # Clamp indices just in case topk returns out-of-bounds due to all -inf
            valid_idx_mask = idx >= 0
            ret_vals = torch.zeros(list(idx.shape) + [self.vals.shape[-1]], device=self.vals.device, dtype=self.vals.dtype)
            if valid_idx_mask.any():
                actual_indices = idx[valid_idx_mask].clamp(min=0, max=self.vals.shape[0]-1) # Clamp only valid ones
                ret_vals[valid_idx_mask] = self.vals[actual_indices]
            return ret_vals, idx


    def inner_update(self, topk_idx: torch.LongTensor, grads_tensor: torch.Tensor, current_lr: float, source: int = SRC_USER_INTERACTION):
        """
        Performs a meta-learning update on the LTM value slots based on the "surprise" gradient.
        Now also updates the timestamp and source metadata for the modified slots.
        """
        with torch.no_grad():
            if grads_tensor is None: return
            device = self.vals.device

            # Filter out invalid indices (e.g., -1 from padding in retrieve_topk)
            valid_mask = topk_idx >= 0
            if not valid_mask.any(): return # No valid indices to update

            idx_flat = topk_idx[valid_mask].view(-1)
            # Ensure grads_tensor has the same shape pattern as topk_idx before masking
            if grads_tensor.shape[:-1] != topk_idx.shape: # Check batch and topk dims only
                print(f"Warning: grads_tensor shape {grads_tensor.shape[:-1]} mismatch with topk_idx shape {topk_idx.shape}. Skipping LTM update.")
                return # Avoid shape errors
            grads_flat = grads_tensor[valid_mask].view(-1, self.vals.size(1))


            slot_grads = torch.zeros_like(self.vals.data)
            slot_grads.index_add_(0, idx_flat.to(device), grads_flat.to(device))

            counts = torch.zeros(self.vals.size(0), device=device)
            counts.index_add_(0, idx_flat.to(device), torch.ones_like(idx_flat, dtype=torch.float, device=device))
            nonzero_mask = counts > 0
            if nonzero_mask.any():
                slot_grads[nonzero_mask] /= counts[nonzero_mask].unsqueeze(-1)

            self._mom_vals.data.mul_(self.momentum).add_(slot_grads)
            update_delta = (self._mom_vals.data + self.weight_decay * self.vals.data)
            update_delta.mul_(-current_lr)

            final_update = torch.zeros_like(self.vals.data)
            # Only apply update where counts > 0 (i.e., where gradients were actually accumulated)
            final_update[nonzero_mask] = update_delta[nonzero_mask]

            # --- UPDATE METADATA ---
            current_time = time.time()
            # Only update metadata for slots that actually received an update
            self.timestamps.data[nonzero_mask] = current_time
            self.sources.data[nonzero_mask] = source
            # --- END METADATA UPDATE ---

            if self.accumulate_deltas:
                # Add the final computed update to both the deltas buffer and the actual values
                self.ltm_deltas.data.add_(final_update)
                self.vals.data.add_(final_update) # Also apply immediately if accumulating
            else:
                self.vals.data.add_(final_update)


class HierarchosCore(nn.Module):
    """The full, trainable hierarchos model, integrating HRM as the core processor."""
    def __init__(self, config: dict):
        super().__init__()
        # Use AttrDict for config to support both dot-notation and .get() method access
        self.config = AttrDict(config)
        # --- Ensure required config values exist ---
        required_keys = ['vocab_size', 'context_dim', 'max_length', 'persistent_dim',
                         'ltm_slots', 'ltm_key_dim', 'ltm_val_dim', 'ltm_lr',
                         'ltm_topk', 'h_hidden', 'l_hidden', 'max_h_steps',
                         'max_l_steps', 'l_conv_atol']
        for key in required_keys:
            if key not in self.config and key != 'max_length':
                raise ValueError(f"Missing required configuration key: '{key}'")
            if key == 'max_length' and not self.config.get('max_length'):
                print("Warning: max_length not found in config during model init. Using default 1024.")
                self.config['max_length'] = 1024

        if 'gradient_checkpointing' not in self.config:
            self.config['gradient_checkpointing'] = False

        self.tok_emb = nn.Embedding(self.config.vocab_size, self.config.context_dim)
        
        # REMOVED: self.pos_emb = nn.Embedding(...) 
        # RWKV relies on internal state and time_decay for position/time information.
        
        self.persistent = nn.Parameter(torch.randn(self.config.persistent_dim) * 0.02)

        # --- FIX: Learnable LTM Gate ---
        # Initialized to -5.0 so sigmoid(-5.0) is approx 0.006 (closed gate).
        self.ltm_gate_logit = nn.Parameter(torch.tensor(-5.0))

        self.ltm = LTMModule(
            n_slots=self.config.ltm_slots,
            key_dim=self.config.ltm_key_dim,
            val_dim=self.config.ltm_val_dim,
            lr=self.config.ltm_lr
        )

        self.qproj = nn.Linear(self.config.context_dim, self.config.ltm_key_dim, bias=False)
        in_dim = self.config.context_dim + self.config.persistent_dim + (self.config.ltm_val_dim * self.config.ltm_topk)
        self.in_proj = nn.Linear(in_dim, self.config.context_dim)

        # RWKV cells
        self.h_rnn = RWKVCell(self.config.h_hidden)
        self.h_to_context = nn.Linear(self.config.h_hidden, self.config.context_dim)

        self.l_input_proj = nn.Linear(self.config.context_dim * 2, self.config.l_hidden)
        self.l_rnn = RWKVCell(self.config.l_hidden)
        self.l_to_out = nn.Linear(self.config.l_hidden, self.config.context_dim)

        self.h_halt_proj = nn.Linear(self.config.h_hidden, 1)
        self.out_norm = nn.LayerNorm(self.config.context_dim)
        self.lm_head = nn.Linear(self.config.context_dim, self.config.vocab_size, bias=False)
        self.tok_emb.weight = self.lm_head.weight  # weight tying

        # Keep torch.compile — you confirmed it's faster even on CPU!
        try:
            if hasattr(torch, "compile"):
                print("INFO: Applying torch.compile to HRM loop (enabled for CPU too!)")
                self._adaptive_hrm_step = torch.compile(
                    self._adaptive_hrm_step,
                    dynamic=True,
                    fullgraph=False,  # safer with variable loop steps
                    options={"triton.cudagraphs": False}
                )
        except Exception as e:
            print(f"WARNING: torch.compile failed ({e}) — falling back to eager mode")

    def _adaptive_hrm_step(self, enc, h_state, l_state):
        """
        Fully torch.compile-safe RWKV-based HRM step.
        Handles fake batch dims from Dynamo tracing automatically via RWKVCell internals.
        """
        # Sanitize inputs in case torch.compile passed 3D/4D wrappers at the start
        if enc.dim() == 3 and enc.shape[0] == 1:
            enc = enc.squeeze(0)
        if h_state.dim() == 4 and h_state.shape[0] == 1:
            h_state = h_state.squeeze(0)
        if l_state.dim() == 4 and l_state.shape[0] == 1:
            l_state = l_state.squeeze(0)

        B = enc.shape[0]
        step_outputs = []
        halt_probs = []
        current_enc = enc

        for _ in range(self.config.max_h_steps):
            h_out, h_state = self.h_rnn(current_enc, h_state)
            
            # RWKVCell now guarantees 2D output [B, H] if input was 2D.
            # But we check just in case compiler re-wrapped it.
            if h_out.dim() == 3 and h_out.shape[0] == 1:
                h_out = h_out.squeeze(0)

            context = self.h_to_context(h_out)  # (B, context_dim)

            # Safe concat — both are now guaranteed (B, D)
            l_input = self.l_input_proj(torch.cat([current_enc, context], dim=-1))

            # L-RWKV loop
            if not self.training:
                l_prev = l_state.clone()
                for _ in range(self.config.max_l_steps):
                    l_out, l_state = self.l_rnn(l_input, l_state)
                    
                    # Use dim=-1 to check the state components for convergence
                    # components: 0=x, 1=aa, 2=bb, 4=sx_cm (skip 3=pp)
                    idx = [0, 1, 2, 4]
                    if torch.allclose(l_state[..., idx], l_prev[..., idx], atol=self.config.l_conv_atol):
                        break
                    l_prev = l_state.clone()
            else:
                for _ in range(self.config.max_l_steps):
                    l_out, l_state = self.l_rnn(l_input, l_state)

            current_enc = current_enc + self.l_to_out(l_out)

            halt_logit = self.h_halt_proj(h_out).squeeze(-1)
            halt_prob = torch.sigmoid(halt_logit)
            step_outputs.append(current_enc)
            halt_probs.append(halt_prob)

            if not self.training and halt_prob.mean() > getattr(self.config, 'h_halt_thresh', 0.9):
                break

        # Ponder cost & weighted average
        if not step_outputs:
            return enc, h_state, l_state, torch.tensor(0.0, device=enc.device)

        step_outputs_t = torch.stack(step_outputs, dim=0)  # (Steps, B, D)
        halt_probs_t   = torch.stack(halt_probs,   dim=0)  # (Steps, B)

        remain = 1.0 - halt_probs_t
        remain_shifted = torch.cat([torch.ones_like(remain[:1]), remain[:-1]], dim=0)
        cum_remain = torch.cumprod(remain_shifted, dim=0)

        weights = halt_probs_t * cum_remain
        remainder = cum_remain[-1] * (1.0 - halt_probs_t[-1])

        total = weights.sum(dim=0) + remainder + 1e-8
        weights = weights / total.unsqueeze(0)
        remainder = remainder / total

        final_enc = (weights.unsqueeze(-1) * step_outputs_t).sum(dim=0) + \
                    remainder.unsqueeze(-1) * step_outputs_t[-1]
        ponder_cost = len(step_outputs) + remainder.mean()

        return final_enc, h_state, l_state, ponder_cost

    def forward(self, input_ids, attention_mask=None, labels=None, min_timestamp=0.0, source_filter=None, **kwargs):
        B, T = input_ids.shape
        device = input_ids.device

        # REMOVED: self.pos_emb(...)
        # Just use token embeddings. RWKV handles position via state decay.
        x = self.tok_emb(input_ids)

        # <<< RWKV states: (B, hidden, 5) >>>
        h_state = torch.zeros(B, self.config.h_hidden, 5, device=device)
        l_state = torch.zeros(B, self.config.l_hidden, 5, device=device)
        h_state[:, :, 3] = -1e30   # pp init
        l_state[:, :, 3] = -1e30

        final_embs = []
        ponder_costs = []

        for t in range(T):
            token_x = x[:, t]
            p = self.persistent.unsqueeze(0).expand(B, -1)
            q = self.qproj(token_x)
            topk_vals, topk_idx = self.ltm.retrieve_topk(q, self.config.ltm_topk, min_timestamp, source_filter)

            # --- FIX: Apply Gate ---
            # Calculate gating factor (0 to 1)
            gate = torch.sigmoid(self.ltm_gate_logit)
            # Scale the retrieved memory values
            gated_vals = topk_vals * gate

            # Use gated_vals for the MAC input
            mac_in = torch.cat([token_x, p, gated_vals.view(B, -1)], dim=-1)
            
            enc = F.gelu(self.in_proj(mac_in))

            if self.config.gradient_checkpointing and self.training:
                enc, h_state, l_state, pc = checkpoint(self._adaptive_hrm_step, enc, h_state, l_state, use_reentrant=False)
            else:
                enc, h_state, l_state, pc = self._adaptive_hrm_step(enc, h_state, l_state)

            final_embs.append(enc)
            ponder_costs.append(pc)

        final = self.out_norm(torch.stack(final_embs, dim=1))
        logits = self.lm_head(final)

        # loss calculation (unchanged)
        loss = None
        ponder_cost_out = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = F.cross_entropy(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
            if ponder_costs:
                ponder_cost_out = torch.stack(ponder_costs).mean()

        return {"loss": loss, "logits": logits, "ponder_cost": ponder_cost_out,
                "topk_vals": topk_vals, "topk_idx": topk_idx}

class Quantizedhierarchos:
    """The quantized hierarchos model for CPU/Vulkan inference (now using RWKV cells)."""
    def __init__(self, config: dict, q_data: dict):
        if not _HAS_KERNEL:
            raise ImportError("Cannot initialize Quantizedhierarchos: C++ kernel not found.")
        self.config = AttrDict(config)
        self.qtype = None

        # Load raw (non-quantized) parameters first (unchanged)
        try:
            self.tok_emb = nn.Embedding.from_pretrained(torch.from_numpy(q_data['tok_emb.weight'].item()['raw']))
            # REMOVED: self.pos_emb = ...
            
            self.persistent = torch.from_numpy(q_data['persistent'].item()['raw'])
            self.out_norm = nn.LayerNorm(self.config.context_dim)
            self.out_norm.load_state_dict({
                'weight': torch.from_numpy(q_data['out_norm.weight'].item()['raw']),
                'bias': torch.from_numpy(q_data['out_norm.bias'].item()['raw'])
            })
            self.ltm = LTMModule(n_slots=self.config.ltm_slots,
                                 key_dim=self.config.ltm_key_dim,
                                 val_dim=self.config.ltm_val_dim)
            # LTM state loading (unchanged)
            ltm_state = {}
            for k in ['ltm.keys', 'ltm.vals', 'ltm.timestamps', 'ltm.sources']:
                if k in q_data:
                    ltm_state[k.split('.', 1)[1]] = torch.from_numpy(q_data[k].item()['raw'])
            self.ltm.load_state_dict(ltm_state, strict=False)
        except Exception as e:
            raise RuntimeError(f"Error loading raw parameters: {e}")

        # ---------- Quantized RWKV layers ----------
        expected_quantized = [
            'qproj', 'in_proj', 'h_rnn', 'h_to_context',
            'l_input_proj', 'l_rnn', 'l_to_out', 'lm_head', 'h_halt_proj'
        ]
        quantized_layers = {}
        for layer_name in expected_quantized:
            if layer_name in ['h_rnn', 'l_rnn']:
                hidden = self.config.h_hidden if layer_name == 'h_rnn' else self.config.l_hidden
                quantized_layers[layer_name] = QuantizedRWKVCell(hidden, layer_name, q_data)
                if self.qtype is None:
                    self.qtype = quantized_layers[layer_name].key.qtype
            else:
                quantized_layers[layer_name] = QuantizedLinear(layer_name, q_data)
                if self.qtype is None:
                    self.qtype = quantized_layers[layer_name].qtype

        # assign to self
        self.qproj          = quantized_layers['qproj']
        self.in_proj        = quantized_layers['in_proj']
        self.h_rnn          = quantized_layers['h_rnn']
        self.h_to_context   = quantized_layers['h_to_context']
        self.l_input_proj   = quantized_layers['l_input_proj']
        self.l_rnn          = quantized_layers['l_rnn']
        self.l_to_out       = quantized_layers['l_to_out']
        self.lm_head        = quantized_layers['lm_head']
        self.h_halt_proj    = quantized_layers['h_halt_proj']

        print(f"Initialized Quantizedhierarchos ({self.qtype}) with RWKV recurrence.")

    def __call__(self, input_ids: torch.LongTensor, h_state: torch.Tensor, l_state: torch.Tensor,
                 device: str = "cpu", min_timestamp: float = 0.0, source_filter: Optional[int] = None):
        B, T = input_ids.shape
        current_pos_start = T - 1 if T > 1 else 0

        for t in range(current_pos_start, T):
            token_ids = input_ids[:, t].cpu().long()
            
            # REMOVED: pos_id logic
            token_emb = self.tok_emb(token_ids) 
            p_read = self.persistent.unsqueeze(0).expand(B, -1)

            query = self.qproj(token_emb, device=device)
            topk_vals, _ = self.ltm.retrieve_topk(query, topk=self.config.ltm_topk,
                                                  min_timestamp=min_timestamp,
                                                  source_filter=source_filter)
            ltm_summary = topk_vals.view(B, -1).cpu()

            mac_input = torch.cat([token_emb.cpu(), p_read.cpu(), ltm_summary], dim=-1)
            enc = F.gelu(self.in_proj(mac_input, device=device))

            # ---- Adaptive HRM with RWKV ----
            for _ in range(self.config.max_h_steps):
                h_out, h_state = self.h_rnn(enc, h_state, device=device)

                halt_logit = self.h_halt_proj(h_out, device=device)
                halt_prob = torch.sigmoid(halt_logit)

                context = self.h_to_context(h_out, device=device)

                l_input_raw = torch.cat([enc.cpu(), context.cpu()], dim=-1)
                l_input = self.l_input_proj(l_input_raw, device=device)

                for _ in range(self.config.max_l_steps):
                    l_out, l_state = self.l_rnn(l_input, l_state, device=device)

                enc = enc + self.l_to_out(l_out, device=device)

                halt_thresh = getattr(self.config, 'h_halt_thresh', 0.9)
                if halt_prob.cpu().mean().item() > halt_thresh:
                    break

        final_embedding = self.out_norm(enc.cpu())
        logits = self.lm_head(final_embedding, device=device)

        return {
            "logits": logits.unsqueeze(1),
            "h_state": h_state.cpu(),
            "l_state": l_state.cpu()
        }

def load_quantized(model_path: str):
    """Loads a quantized model directory, automatically finding the .npz and tokenizer."""
    if not _HAS_KERNEL:
        raise ImportError("Cannot load quantized model: C++ kernel not found.")

    print(f"Loading quantized model from directory: {model_path}")

    # Find the .npz file in the directory
    npz_files = [f for f in os.listdir(model_path) if f.endswith('.npz')]
    if not npz_files:
        raise FileNotFoundError(f"No quantized model .npz file found in {model_path}")
    if len(npz_files) > 1:
        print(f"Warning: Multiple .npz files found. Using the first one: {npz_files[0]}")

    weights_path = os.path.join(model_path, npz_files[0])
    q_data = np.load(weights_path, allow_pickle=True)

    if '_config' not in q_data:
        raise ValueError("Quantized model file is missing '_config' data. Please re-quantize the model.")

    config_dict = q_data['_config'].item() # Load config dict from npz
    # Ensure config gets 'model_type' if missing, useful for HF compatibility downstream
    if 'model_type' not in config_dict:
        config_dict['model_type'] = 'hierarchos'

    config = AttrDict(config_dict) # Convert to AttrDict

    return Quantizedhierarchos(config, q_data), config # Return both object and AttrDict config


def load_full_model_with_config(model_path: str, device):
    """Loads a full-precision model and its config from a directory."""
    weights_path = os.path.join(model_path, MODEL_WEIGHTS_NAME)
    if not os.path.exists(weights_path):
        raise FileNotFoundError(f"Model weights file '{MODEL_WEIGHTS_NAME}' not found in '{model_path}'")

    # Load checkpoint safely, allowing pickles only if necessary (e.g., for optimizer state)
    try:
        # Try weights_only=True first for security if config allows
        checkpoint = torch.load(weights_path, map_location=device, weights_only=True)
        # Verify config is present
        if 'config' not in checkpoint:
            print("INFO: Config not found in weights_only load. Retrying with weights_only=False.")
            checkpoint = torch.load(weights_path, map_location=device, weights_only=False) # Allow pickles if needed
    except RuntimeError as e: # Catch errors potentially related to weights_only=True
        print(f"Warning: Failed to load checkpoint with weights_only=True ({e}). Retrying with weights_only=False (allowing pickles).")
        try:
            checkpoint = torch.load(weights_path, map_location=device, weights_only=False)
        except Exception as inner_e:
            raise RuntimeError(f"Failed to load checkpoint even allowing pickles: {inner_e}")
    except Exception as e: # Catch other loading errors
        raise RuntimeError(f"Failed to load checkpoint: {e}")


    # Config must be present now
    if 'config' not in checkpoint:
        raise ValueError("Model config not found in checkpoint. The model file is likely corrupted or from an old version.")

    config_dict = checkpoint['config'] # Config is likely a dict
    # Ensure model_type is present for HuggingFace compatibility
    if 'model_type' not in config_dict:
        config_dict['model_type'] = 'hierarchos'

    # Ensure vocab_size is present before creating model
    if 'vocab_size' not in config_dict:
        raise ValueError("Cannot initialize model: 'vocab_size' missing from checkpoint config.")

    config = AttrDict(config_dict) # Convert to AttrDict for model init


    model = HierarchosCore(config).to(device) # Pass AttrDict config to model

    # Load state dict, be flexible with missing/extra keys if needed
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=True)
    except RuntimeError as e:
        print(f"Warning: Non-strict state dict loading due to mismatch: {e}. Trying strict=False.")
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)


    return model, config # Return model and AttrDict config


def train(args, device, tokenizer, dataloader, dataloader_len): # <<< Pass dataloader in
    print("Running in TRAIN mode...")
    config = vars(args) # Start with CLI args
    # Ensure train data path or HF dataset name is saved in config
    if args.hf_dataset:
        config['hf_dataset'] = args.hf_dataset
        config['hf_dataset_config'] = args.hf_dataset_config
        config['hf_dataset_split'] = args.hf_dataset_split
    else:
        config['train_data_path'] = args.train
    config['model_type'] = 'hierarchos' # Ensure model_type is set
    # <<< MODIFIED: Save dataset type flags in config >>>
    config['pre_chunked_dataset'] = args.pre_chunked_dataset
    config['pre_pt_dataset'] = args.pre_pt_dataset
    config['is_hf_dataset'] = bool(args.hf_dataset) # Save flag for HF dataset usage
    # <<< NEW: Add gradient checkpointing flag to initial config >>>
    config['gradient_checkpointing'] = args.gradient_checkpointing

    # --- Determine vocab_size (already handled during tokenizer load) ---
    current_vocab_size = len(tokenizer) if tokenizer else None

    model = None # Initialize model variable
    optimizer = None # Initialize optimizer variable
    start_epoch = 0
    model_config = None # Initialize model_config
    scaler = None # Initialize scaler
    scheduler = None # Initialize scheduler
    use_amp = args.amp and _HAS_AMP # Determine AMP usage early

    # --- Dataloader creation moved to main() ---

    # --- Handle starting from an existing model directory ---
    if args.model_path and not args.resume_from_ckpt:
        print(f"INFO: Starting training using initial weights from model directory: {args.model_path}")
        try:
            # Load the model and its config. This should be an inference checkpoint.
            model, model_config = load_full_model_with_config(args.model_path, device)

            # Check vocab size consistency
            if current_vocab_size is not None and model_config.vocab_size != current_vocab_size:
                print(f"Warning: Loaded model vocab_size ({model_config.vocab_size}) differs from tokenizer ({current_vocab_size}). Using model's value.")
            elif 'vocab_size' not in model_config and current_vocab_size:
                print(f"Warning: 'vocab_size' missing from loaded model config. Using tokenizer's value ({current_vocab_size}).")
                model_config.vocab_size = current_vocab_size # Patch config
                # Re-initialize parts affected by vocab_size if necessary (tok_emb, lm_head)
                model.tok_emb = nn.Embedding(model_config.vocab_size, model_config.context_dim).to(device)
                model.lm_head = nn.Linear(model_config.context_dim, model_config.vocab_size, bias=False).to(device)
                model.tok_emb.weight = model.lm_head.weight # Re-tie weights
            elif 'vocab_size' not in model_config:
                raise ValueError("Cannot determine vocab_size: Not found in loaded model config and tokenizer not available.")

            # <<< Ensure max_length from CLI is used if provided, otherwise from loaded config >>>
            if args.max_length and args.max_length != model_config.max_length:
                 print(f"INFO: Overriding loaded model max_length ({model_config.max_length}) with CLI value ({args.max_length}).")
                 model_config.max_length = args.max_length
                 # Re-init pos_emb if necessary
                 model.pos_emb = nn.Embedding(model_config.max_length, model_config.context_dim).to(device)
            elif 'max_length' not in model_config and args.max_length:
                 print(f"INFO: max_length missing from loaded config. Using CLI value ({args.max_length}).")
                 model_config.max_length = args.max_length
                 model.pos_emb = nn.Embedding(model_config.max_length, model_config.context_dim).to(device)
            elif 'max_length' not in model_config:
                 print(f"Warning: max_length missing from loaded config and CLI. Using default 1024.")
                 model_config.max_length = 1024
                 model.pos_emb = nn.Embedding(model_config.max_length, model_config.context_dim).to(device)

            # <<< NEW: Ensure gradient_checkpointing flag from CLI is used >>>
            if args.gradient_checkpointing != model_config.get('gradient_checkpointing', False):
                 print(f"INFO: Overriding loaded model gradient_checkpointing ({model_config.get('gradient_checkpointing', False)}) with CLI value ({args.gradient_checkpointing}).")
                 model_config.gradient_checkpointing = args.gradient_checkpointing
            elif 'gradient_checkpointing' not in model_config:
                 model_config.gradient_checkpointing = args.gradient_checkpointing # Add if missing

            # Update the model's config in case it was modified
            model.config = model_config


            # --- Initialize optimizer, scaler, scheduler FRESH ---
            print("INFO: Initializing optimizer, scheduler, and scaler from scratch.")
            optimizer = ADAM_OPTIMIZER(model.parameters(), lr=args.starting_lr)
            if use_amp:
                scaler = GradScaler()
                print("INFO: Automatic Mixed Precision (AMP) ENABLED for training.")

            num_update_steps = (dataloader_len // args.accumulation_steps) * args.epochs if dataloader_len > 0 else 0
            if not args.disable_lr_schedule and num_update_steps > 0:
                print(f"INFO: Step-based Cosine Annealing LR scheduler ENABLED. Total update steps: {num_update_steps}, Max LR: {args.starting_lr}, Min LR: {args.min_lr}")
                scheduler = CosineAnnealingLR(optimizer, T_max=num_update_steps, eta_min=args.min_lr)
            # start_epoch remains 0

        except FileNotFoundError:
            print(f"ERROR: --model-path specified ({args.model_path}), but it does not seem to contain a valid model directory.")
            sys.exit(1)
        except Exception as e:
            print(f"ERROR: Failed to load model from --model-path ({args.model_path}): {e}")
            traceback.print_exc()
            sys.exit(1)

    # <<< Resume logic >>>
    elif args.resume_from_ckpt:
        if not os.path.exists(args.resume_from_ckpt):
            raise FileNotFoundError(f"Checkpoint to resume from not found at {args.resume_from_ckpt}")

        print(f"Resuming training from checkpoint: {args.resume_from_ckpt}")

        # weights_only=False is crucial for loading optimizer, scheduler, scaler
        try:
            checkpoint = torch.load(args.resume_from_ckpt, map_location=device, weights_only=False)
        except Exception as e:
            raise RuntimeError(f"Failed to load training checkpoint (needs optimizer/scheduler state): {e}")


        if 'optimizer_state_dict' not in checkpoint:
            raise ValueError("The specified checkpoint is a final inference model, not a training checkpoint. Cannot resume.")

        # Load config from checkpoint to ensure consistency
        if 'config' in checkpoint:
            model_config = AttrDict(checkpoint['config'])
            # Patch vocab_size if missing or different
            if 'vocab_size' not in model_config:
                if current_vocab_size:
                    print(f"Warning: 'vocab_size' not found in checkpoint config. Setting from loaded tokenizer ({current_vocab_size}).")
                    model_config['vocab_size'] = current_vocab_size
                else:
                    raise ValueError("Cannot determine vocab_size: Not found in checkpoint and tokenizer not loaded.")
            elif current_vocab_size is not None and model_config.vocab_size != current_vocab_size:
                print(f"Warning: Checkpoint vocab_size ({model_config.vocab_size}) differs from loaded tokenizer ({current_vocab_size}). Using checkpoint value.")

            # <<< Ensure max_length consistency, prioritize CLI arg >>>
            if args.max_length and args.max_length != model_config.max_length:
                 print(f"INFO: Overriding checkpoint max_length ({model_config.max_length}) with CLI value ({args.max_length}).")
                 model_config.max_length = args.max_length
            elif 'max_length' not in model_config and args.max_length:
                 print(f"INFO: max_length missing from checkpoint config. Using CLI value ({args.max_length}).")
                 model_config.max_length = args.max_length
            elif 'max_length' not in model_config:
                 print(f"Warning: max_length missing from checkpoint config and CLI. Using default 1024.")
                 model_config.max_length = 1024

            # <<< Ensure gradient_checkpointing consistency, prioritize CLI arg >>>
            if args.gradient_checkpointing != model_config.get('gradient_checkpointing', False):
                 print(f"INFO: Overriding checkpoint gradient_checkpointing ({model_config.get('gradient_checkpointing', False)}) with CLI value ({args.gradient_checkpointing}).")
                 model_config.gradient_checkpointing = args.gradient_checkpointing
            elif 'gradient_checkpointing' not in model_config:
                 model_config.gradient_checkpointing = args.gradient_checkpointing # Add if missing


            # Ensure model_type is present for HuggingFace compatibility
            if 'model_type' not in model_config:
                model_config['model_type'] = 'hierarchos'

            print("INFO: Re-initializing model architecture from checkpoint config.")
            model = HierarchosCore(model_config).to(device) # Create model AFTER potentially fixing vocab_size/max_length/grad_ckpt
        else:
            print("Warning: Config not found in checkpoint. Using current CLI args for model architecture.")
            cli_config = config # Use the initial config from vars(args)
            if 'vocab_size' not in cli_config and current_vocab_size:
                cli_config['vocab_size'] = current_vocab_size
            elif 'vocab_size' not in cli_config:
                raise ValueError("Cannot determine vocab_size: Not found in checkpoint or CLI args, and tokenizer not loaded.")
            # <<< Ensure max_length is set in cli_config >>>
            if 'max_length' not in cli_config and args.max_length:
                cli_config['max_length'] = args.max_length
            elif 'max_length' not in cli_config:
                cli_config['max_length'] = 1024 # Default
            # <<< Ensure gradient_checkpointing is set in cli_config >>>
            cli_config['gradient_checkpointing'] = args.gradient_checkpointing

            model_config = AttrDict(cli_config) # Fallback, might cause issues if arch changed
            model = HierarchosCore(model_config).to(device)


        # --- Optimizer Initialization/Loading Logic ---
        initial_lr_for_optim = args.starting_lr if args.override_scheduling else model_config.get('starting_lr', args.starting_lr)
        optimizer = ADAM_OPTIMIZER(model.parameters(), lr=initial_lr_for_optim)

        # Load model state dict with flexibility
        try:
            model.load_state_dict(checkpoint['model_state_dict'], strict=True)
        except RuntimeError as e:
            print(f"Warning: Non-strict model state dict loading: {e}. Trying strict=False.")
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)

        # --- Conditional Optimizer State Loading ---
        if not args.override_scheduling:
            try:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            except Exception as e:
                print(f"Warning: Could not load optimizer state: {e}. Starting optimizer from scratch.")
                optimizer = ADAM_OPTIMIZER(model.parameters(), lr=initial_lr_for_optim) # Re-init with correct LR
        else:
            print("INFO: --override-scheduling detected. Skipping loading optimizer state.")

        start_epoch = checkpoint.get('completed_epoch', 0) # Use get for safety

        # --- Initialize AMP GradScaler ---
        if use_amp:
            scaler = GradScaler()
            print("INFO: Automatic Mixed Precision (AMP) ENABLED for training.")
            # Resume GradScaler state
            if 'scaler_state_dict' in checkpoint and checkpoint['scaler_state_dict'] is not None and not args.override_scheduling:
                print("Resuming GradScaler state.")
                try:
                    scaler.load_state_dict(checkpoint['scaler_state_dict'])
                except Exception as e:
                    print(f"Warning: Failed to load scaler state: {e}. Continuing with a fresh scaler.")
            elif not args.override_scheduling:
                # Only warn if not overriding and state is missing
                if 'scaler_state_dict' not in checkpoint or checkpoint['scaler_state_dict'] is None:
                    print("Warning: Scaler state not found in checkpoint. Initializing a fresh scaler.")
            elif args.override_scheduling:
                print("INFO: --override-scheduling set. Initializing a fresh scaler.")


        # --- Initialize Scheduler (AFTER optimizer is potentially re-initialized) ---
        num_update_steps = (dataloader_len // args.accumulation_steps) * args.epochs if dataloader_len > 0 else 0

        if not args.disable_lr_schedule and num_update_steps > 0:
            # Use current args.starting_lr and args.min_lr when initializing scheduler
            print(f"INFO: Step-based Cosine Annealing LR scheduler ENABLED. Total update steps: {num_update_steps}, Max LR: {args.starting_lr}, Min LR: {args.min_lr}")
            scheduler = CosineAnnealingLR(optimizer, T_max=num_update_steps, eta_min=args.min_lr)

            # --- Scheduler Resuming Logic ---
            checkpoint_has_scheduler = 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] is not None

            if checkpoint_has_scheduler and not args.override_scheduling:
                # Load old state but WARN if LR args have changed without override flag
                old_lr = model_config.get('starting_lr')
                old_min_lr = model_config.get('min_lr')
                lr_mismatch = (old_lr is not None and not np.isclose(old_lr, args.starting_lr))
                min_lr_mismatch = (old_min_lr is not None and not np.isclose(old_min_lr, args.min_lr))

                if lr_mismatch or min_lr_mismatch:
                    print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                    print("!!! WARNING: New LR flags detected but --override-scheduling was not set.             !!!")
                    print(f"!!!  Your new LR ({args.starting_lr}) / Min LR ({args.min_lr}) WILL BE IGNORED.                  !!!")
                    print(f"!!!  Loading old schedule state (LR: {old_lr}, Min LR: {old_min_lr}).                      !!!")
                    print("!!!  To use your new LR flags, add --override-scheduling to your command.           !!!")
                    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")

                print("Resuming learning rate scheduler state.")
                try:
                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    # Crucially, update optimizer's LR to match the resumed scheduler's state
                    # This handles cases where the saved optimizer state had a different LR
                    for i, param_group in enumerate(optimizer.param_groups):
                        param_group['lr'] = scheduler.get_last_lr()[i]

                except Exception as e:
                    print(f"Warning: Failed to load scheduler state: {e}. Continuing with potentially incorrect LR.")

            elif args.override_scheduling or not checkpoint_has_scheduler:
                if args.override_scheduling and checkpoint_has_scheduler:
                    print("INFO: --override-scheduling detected. Ignoring checkpoint's scheduler state and using new LR args.")
                elif not checkpoint_has_scheduler:
                    print("Warning: No scheduler state found in checkpoint. Initializing new schedule based on new LR args.")

                # Set the step count to where it should be for the resumed epoch
                steps_per_epoch = dataloader_len // args.accumulation_steps if dataloader_len > 0 else 0
                # last_epoch should be the number of steps *completed*
                scheduler.last_epoch = max(-1, start_epoch * steps_per_epoch -1) # -1 if starting epoch 0
                print(f"INFO: Setting scheduler last_epoch to {scheduler.last_epoch} based on resumed epoch {start_epoch}.")


        print(f"Successfully loaded model state. Resuming from epoch {start_epoch + 1}.")


    # <<< Starting completely fresh >>>
    else:
        print("INFO: Starting training from scratch (no --resume-from-ckpt or --model-path provided).")
        # Need to ensure vocab_size is in the config used to create the model
        if 'vocab_size' not in config:
            if current_vocab_size:
                config['vocab_size'] = current_vocab_size
            else:
                raise ValueError("Cannot determine vocab_size for new model.")
        # Ensure max_length is set in config
        if 'max_length' not in config or config['max_length'] is None:
            if args.max_length:
                config['max_length'] = args.max_length
            else:
                # Should have defaulted or been set by auto-scan by now
                raise ValueError("max_length not determined for new model (use --max_length or --auto-max-length).")

        # <<< NEW: gradient_checkpointing is already in config from vars(args) >>>

        model = HierarchosCore(config).to(device)
        optimizer = ADAM_OPTIMIZER(model.parameters(), lr=args.starting_lr)
        model_config = AttrDict(config) # Use the potentially updated CLI args config

        # --- Initialize AMP GradScaler ---
        if use_amp:
            scaler = GradScaler()
            print("INFO: Automatic Mixed Precision (AMP) ENABLED for training.")
        # --- Initialize Scheduler ---
        num_update_steps = (dataloader_len // args.accumulation_steps) * args.epochs if dataloader_len > 0 else 0
        if not args.disable_lr_schedule and num_update_steps > 0:
            print(f"INFO: Step-based Cosine Annealing LR scheduler ENABLED. Total update steps: {num_update_steps}, Max LR: {args.starting_lr}, Min LR: {args.min_lr}")
            scheduler = CosineAnnealingLR(optimizer, T_max=num_update_steps, eta_min=args.min_lr)


    # --- Start Training Loop ---
    model.train()
    print(f"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    os.makedirs(args.out_dir, exist_ok=True)

    # Zero gradients *before* starting the loop, especially important when resuming
    optimizer.zero_grad(set_to_none=True)

    global_step = 0 # Track global steps for iterable datasets and scheduler

    for epoch in range(start_epoch, args.epochs):
        print(f"\n--- Epoch {epoch + 1} / {args.epochs} ---")
        pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1}")
        total_loss = 0.0
        total_ponder_cost = 0.0
        # Flag to track if backward was called in the current accumulation cycle
        backward_called_in_cycle = False
        steps_in_epoch = 0 # Track steps for averaging

        for i, batch in enumerate(pbar):

            # ==========================================================
            # --- ❗️❗️❗️ BEGIN KEY FIX ❗️❗️❗️ ---
            # ==========================================================
            # Tell the compiler that a new step is beginning.
            # This fixes the CUDAGraphs + Checkpointing conflict.
            #if device.type == 'cuda': # Only needed for CUDAGraphs
            #    cudagraph_mark_step_begin()
            # ==========================================================
            # --- ❗️❗️❗️ END KEY FIX ❗️❗️❗️ ---
            # ==========================================================

            # Handle potential None batch from collate_fn if it was empty
            if batch is None:
                print(f"Warning: Skipping empty batch at step {i}.")
                continue

            input_ids, attention_mask, labels = batch["input_ids"].to(device), batch["attention_mask"].to(device), batch["labels"].to(device)

            # --- AMP autocast context ---
            # <<< FIXED: Added device_type >>>
            with autocast(device_type=device.type, enabled=use_amp):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                cross_entropy_loss = outputs["loss"]
                ponder_cost = outputs["ponder_cost"]

                combined_loss = None
                # Check for valid loss components BEFORE combining
                ce_valid = cross_entropy_loss is not None and not torch.isnan(cross_entropy_loss) and not torch.isinf(cross_entropy_loss)
                pc_valid = ponder_cost is not None and not torch.isnan(ponder_cost) and not torch.isinf(ponder_cost)

                if ce_valid and pc_valid:
                    combined_loss = cross_entropy_loss + args.ponder_loss_weight * ponder_cost
                elif ce_valid: # Only CE is valid
                    if i % args.accumulation_steps == 0: # Print only once per update step
                        print(f"\nWarning: Ponder cost is NaN/Inf at step {i+1}. Using only CrossEntropy loss for this step.")
                    combined_loss = cross_entropy_loss
                elif not ce_valid: # CE loss is invalid, skip backward
                    if i % args.accumulation_steps == 0:
                        print(f"\nWarning: CrossEntropy loss is NaN/Inf at step {i+1}. Skipping backward pass for this step.")
                    combined_loss = None # Ensure it's None


            # --- Backward Pass ---
            if combined_loss is not None:
                loss_to_backward = combined_loss / args.accumulation_steps

                if use_amp:
                    # scaler.scale automatically checks for enabled state
                    scaler.scale(loss_to_backward).backward()
                else:
                    loss_to_backward.backward()

                backward_called_in_cycle = True # Mark that backward was successful

                # Accumulate display stats only if loss was valid
                if ce_valid:
                    total_loss += cross_entropy_loss.item()
                if pc_valid:
                    total_ponder_cost += ponder_cost.item()
                steps_in_epoch += 1 # Count step only if loss was valid and backward called


            # --- Optimizer Step (End of Accumulation Cycle) ---
            if (i + 1) % args.accumulation_steps == 0:
                # Only proceed if backward was called at least once in this cycle
                if backward_called_in_cycle:
                    # --- LTM Update (Before Optimizer Step) ---
                    ltm_grads = None
                    # Ensure topk_vals exists and requires grad before accessing .grad
                    if outputs.get("topk_vals") is not None and outputs["topk_vals"].requires_grad and outputs["topk_vals"].grad_fn is not None:
                        # Check grad existence, backward() might not populate it if detached earlier
                        if outputs["topk_vals"].grad is not None:
                            ltm_grads = outputs["topk_vals"].grad
                        # else: # Optional warning
                        #     print(f"\nWarning: LTM topk_vals.grad is None at step {i+1}, skipping LTM update.")

                    if ltm_grads is not None:
                        # Make a copy before potentially modifying in-place with unscaling
                        ltm_grads_copy = ltm_grads.detach().clone()
                        if use_amp:
                            # Manually unscale LTM grads *if* the scaler is currently scaled
                            current_scale = scaler.get_scale()
                            if current_scale != 1.0: # Check if scaling is active
                                if scaler._enabled and scaler._scale is not None:
                                    assert current_scale > 0.0 # Should always be true if scale != 1.0
                                    ltm_grads_copy = ltm_grads_copy / current_scale # Unscale the copy
                                else: # If scaler somehow disabled or scale is None, don't unscale
                                    print(f"\nWarning: Scaler state inconsistent at step {i+1}, cannot unscale LTM grads.")

                        # Use the LTM LR specified in args for updates during training
                        model.ltm.inner_update(outputs["topk_idx"], ltm_grads_copy, current_lr=args.ltm_lr, source=LTMModule.SRC_TRAINING_DATA)


                    # --- Optimizer Step ---
                    if use_amp:
                        # Unscale gradients - performs inf/NaN checks
                        scaler.unscale_(optimizer)

                        if args.grad_clip > 0:
                            # Clip gradients *after* unscaling
                            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

                        # Step the optimizer - scaler skips if unscale_ found inf/NaN
                        scaler.step(optimizer)

                        # Update the scale factor for the next iteration
                        scaler.update()
                    else: # Not using AMP
                        if args.grad_clip > 0:
                            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
                        optimizer.step()

                    # Step the learning rate scheduler *after* the optimizer step
                    if scheduler:
                        scheduler.step()

                    # Zero gradients for the next accumulation cycle
                    optimizer.zero_grad(set_to_none=True)

                    # Reset flag for the next cycle
                    backward_called_in_cycle = False
                    global_step += 1 # Increment global step after optimizer step

                else: # Backward was not called in this cycle (all losses were NaN/Inf)
                    print(f"\nWarning: Skipping optimizer step at batch {i+1} due to invalid loss(es) in accumulation cycle.")
                    # Still need to zero gradients that might exist from previous cycles if resuming
                    optimizer.zero_grad(set_to_none=True)
                    backward_called_in_cycle = False # Reset flag

            # --- Update Progress Bar ---
            # Use steps_in_epoch for averaging loss/ponder
            avg_loss = total_loss / steps_in_epoch if steps_in_epoch > 0 else 0.0
            avg_ponder = total_ponder_cost / steps_in_epoch if steps_in_epoch > 0 else 0.0
            current_lr = scheduler.get_last_lr()[0] if scheduler else args.starting_lr
            pbar.set_postfix({
                "loss": f"{avg_loss:.4f}",
                "ponder": f"{avg_ponder:.2f}",
                "lr": f"{current_lr:.2e}"
            })

        # --- End of Epoch ---
        ckpt_path = os.path.join(args.out_dir, f"hierarchos_epoch_{epoch + 1}.pt")
        print(f"Epoch {epoch + 1} complete. Saving training checkpoint to {ckpt_path}")

        # Ensure config saved reflects current state (including potential patches)
        config_to_save = dict(model.config) # Get config directly from model
        config_to_save['starting_lr'] = args.starting_lr # Use current CLI args for these
        config_to_save['min_lr'] = args.min_lr
        config_to_save['disable_lr_schedule'] = args.disable_lr_schedule
        # Save data source info
        if args.hf_dataset:
            config_to_save['hf_dataset'] = args.hf_dataset
            config_to_save['hf_dataset_config'] = args.hf_dataset_config
            config_to_save['hf_dataset_split'] = args.hf_dataset_split
        else:
            config_to_save['train_data_path'] = args.train # Save train path used
        # <<< MODIFIED: Save dataset type flags >>>
        config_to_save['pre_chunked_dataset'] = args.pre_chunked_dataset
        config_to_save['pre_pt_dataset'] = args.pre_pt_dataset
        config_to_save['is_hf_dataset'] = bool(args.hf_dataset)
        config_to_save['kayla'] = args.kayla # Save kayla mode used
        # <<< NEW: Save gradient checkpointing flag >>>
        config_to_save['gradient_checkpointing'] = args.gradient_checkpointing
        # Ensure vocab_size is saved (should be in model.config by now)
        if 'vocab_size' not in config_to_save:
            print(f"CRITICAL WARNING: vocab_size missing from model config before saving epoch {epoch+1} checkpoint!")
        # <<< Ensure max_length is saved >>>
        if 'max_length' not in config_to_save:
             print(f"CRITICAL WARNING: max_length missing from model config before saving epoch {epoch+1} checkpoint!")


        # Prepare state dicts for saving
        scaler_state = scaler.state_dict() if use_amp and scaler is not None else None
        scheduler_state = scheduler.state_dict() if scheduler is not None else None

        torch.save({
            'completed_epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler_state,
            'scaler_state_dict': scaler_state,
            'config': config_to_save, # Save potentially updated config
        }, ckpt_path)

    # --- End of Training ---
    final_save_path = os.path.join(args.out_dir, MODEL_WEIGHTS_NAME)
    print(f"\nTraining finished. Saving final inference model to {final_save_path}")

    # Ensure final config is saved correctly
    final_config_to_save = dict(model.config)
    final_config_to_save['starting_lr'] = args.starting_lr
    final_config_to_save['min_lr'] = args.min_lr
    final_config_to_save['disable_lr_schedule'] = args.disable_lr_schedule
    # Save data source info
    if args.hf_dataset:
        final_config_to_save['hf_dataset'] = args.hf_dataset
        final_config_to_save['hf_dataset_config'] = args.hf_dataset_config
        final_config_to_save['hf_dataset_split'] = args.hf_dataset_split
    else:
        final_config_to_save['train_data_path'] = args.train
    # <<< MODIFIED: Save dataset type flags >>>
    final_config_to_save['pre_chunked_dataset'] = args.pre_chunked_dataset
    final_config_to_save['pre_pt_dataset'] = args.pre_pt_dataset
    final_config_to_save['is_hf_dataset'] = bool(args.hf_dataset)
    final_config_to_save['kayla'] = args.kayla
    # <<< NEW: Save gradient checkpointing flag >>>
    final_config_to_save['gradient_checkpointing'] = args.gradient_checkpointing
    if 'vocab_size' not in final_config_to_save:
        print(f"CRITICAL WARNING: vocab_size missing from model config before saving final model!")
    if 'max_length' not in final_config_to_save:
        print(f"CRITICAL WARNING: max_length missing from model config before saving final model!")


    torch.save({
        'model_state_dict': model.state_dict(),
        'config': final_config_to_save
    }, final_save_path)

    try:
        if tokenizer:
            tokenizer.save_pretrained(args.out_dir)
            print(f"Tokenizer files saved to {args.out_dir}")
    except Exception as e:
        print(f"Warning: Failed to save tokenizer files on completion. Error: {e}")


    if args.quantize_on_complete:
        print("\n--- Training Complete: Starting On-the-Fly Quantization ---")
        # Quantize to a new directory for clarity, e.g., './my_model-INT4'
        quantize_out_dir = args.out_dir.rstrip('/\\') + f"-{args.qtype}"
        quantize(args, device, model, tokenizer, quantize_out_dir)


# <<< FINETUNE Function (modified to accept dataloader) >>>
def finetune(args, device, tokenizer, dataloader, dataloader_len): # <<< Pass dataloader in
    if not _HAS_PEFT: raise ImportError("Please install 'peft' for fine-tuning.")
    print("Running in FINETUNE mode with LoRA...")

    # Load the base model and its config from the specified directory
    model, model_config = load_full_model_with_config(args.model_path, device)

    # <<< Ensure max_length from CLI is used if provided >>>
    if args.max_length and args.max_length != model_config.max_length:
         print(f"INFO: Overriding loaded model max_length ({model_config.max_length}) with CLI value ({args.max_length}) for finetuning.")
         model_config.max_length = args.max_length
         model.pos_emb = nn.Embedding(model_config.max_length, model_config.context_dim).to(device)
    elif 'max_length' not in model_config:
         print(f"Warning: max_length missing from loaded config. Using default 1024 for finetuning.")
         model_config.max_length = 1024 # Or use args.max_length if provided
         model.pos_emb = nn.Embedding(model_config.max_length, model_config.context_dim).to(device)

    # <<< NEW: Ensure gradient_checkpointing flag from CLI is used >>>
    if args.gradient_checkpointing != model_config.get('gradient_checkpointing', False):
         print(f"INFO: Overriding loaded model gradient_checkpointing ({model_config.get('gradient_checkpointing', False)}) with CLI value ({args.gradient_checkpointing}) for finetuning.")
         model_config.gradient_checkpointing = args.gradient_checkpointing
    elif 'gradient_checkpointing' not in model_config:
         model_config.gradient_checkpointing = args.gradient_checkpointing # Add if missing
    # Update the model's config
    model.config = model_config


    lora_r = args.lora_r
    if args.finetune_unlock_percent is not None: # Check if flag was actually used
        if args.lora_r != 8: # Default value check
            print(f"Warning: Both --lora_r ({args.lora_r}) and --finetune-unlock-percent were specified. Prioritizing --lora_r.")
        else:
            total_params = sum(p.numel() for p in model.parameters())
            target_modules = ["qproj", "in_proj", "h_to_context", "l_to_out", "h_halt_proj", "W_ir", "W_hr", "W_iz", "W_hz", "W_in", "W_hn"] # Include GRU weights
            lora_param_sum_per_r = 0
            for name, module in model.named_modules():
                if isinstance(module, nn.Linear) and any(tm in name for tm in target_modules):
                    lora_param_sum_per_r += module.in_features + module.out_features

            target_trainable_count = total_params * (args.finetune_unlock_percent / 100.0)
            if lora_param_sum_per_r > 0:
                estimated_r = target_trainable_count / lora_param_sum_per_r
                lora_r = max(1, int(round(estimated_r)))
                print(f"Targeting ~{args.finetune_unlock_percent}% trainable parameters. Estimated LoRA rank 'r' = {lora_r}")
            else:
                print("Warning: Could not find target modules for LoRA. Using default r=8.")

    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=[
            # RWKV time-mixing
            "key", "value", "receptance", "output",
            # RWKV channel-mixing
            "key_cm", "receptance_cm", "value_cm",
            # Hierarchos-specific layers
            "qproj", "in_proj", "h_to_context",
            "l_input_proj", "l_to_out", "h_halt_proj"
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        modules_to_save=["ltm"],  # LTM still updated directly
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    optimizer = ADAM_OPTIMIZER(model.parameters(), lr=args.starting_lr) # Only trainable params will have grads
    os.makedirs(args.out_dir, exist_ok=True)

    scaler = None
    use_amp = args.amp and _HAS_AMP
    if use_amp:
        scaler = GradScaler()
        print("INFO: Automatic Mixed Precision (AMP) ENABLED for fine-tuning.")

    scheduler = None
    if not args.disable_lr_schedule:
        num_update_steps = (dataloader_len // args.accumulation_steps) * args.epochs if dataloader_len > 0 else 0
        if num_update_steps > 0:
            print(f"INFO: Step-based Cosine Annealing LR scheduler ENABLED for finetuning. Total update steps: {num_update_steps}, Max LR: {args.starting_lr}, Min LR: {args.min_lr}")
            scheduler = CosineAnnealingLR(optimizer, T_max=num_update_steps, eta_min=args.min_lr)
        else:
            print("Warning: Cannot enable LR schedule, dataset might be too small or empty.")


    optimizer.zero_grad(set_to_none=True)
    global_step = 0 # Track global steps for scheduler

    for epoch in range(args.epochs):
        print(f"\n--- LoRA Finetune Epoch {epoch + 1} / {args.epochs} ---")
        pbar = tqdm(dataloader, desc=f"Finetune Epoch {epoch + 1}")
        total_loss = 0.0
        total_ponder_cost = 0.0
        backward_called_in_cycle = False
        steps_in_epoch = 0

        for i, batch in enumerate(pbar):
            if batch is None: continue # Skip empty batches

            input_ids, attention_mask, labels = batch["input_ids"].to(device), batch["attention_mask"].to(device), batch["labels"].to(device)

            # <<< FIXED: Added device_type >>>
            with autocast(device_type=device.type, enabled=use_amp):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                cross_entropy_loss = outputs["loss"]
                ponder_cost = outputs["ponder_cost"]

                combined_loss = None
                ce_valid = cross_entropy_loss is not None and not torch.isnan(cross_entropy_loss) and not torch.isinf(cross_entropy_loss)
                pc_valid = ponder_cost is not None and not torch.isnan(ponder_cost) and not torch.isinf(ponder_cost)

                if ce_valid and pc_valid:
                    combined_loss = cross_entropy_loss + args.ponder_loss_weight * ponder_cost
                elif ce_valid:
                    if i % args.accumulation_steps == 0:
                        print(f"\nWarning: Ponder cost is NaN/Inf at step {i+1}. Using only CrossEntropy loss.")
                    combined_loss = cross_entropy_loss
                elif not ce_valid:
                    if i % args.accumulation_steps == 0:
                        print(f"\nWarning: CrossEntropy loss is NaN/Inf at step {i+1}. Skipping backward pass.")
                    combined_loss = None


            if combined_loss is not None:
                loss_to_backward = combined_loss / args.accumulation_steps

                if use_amp:
                    scaler.scale(loss_to_backward).backward()
                else:
                    loss_to_backward.backward()

                backward_called_in_cycle = True

                if ce_valid: total_loss += cross_entropy_loss.item()
                if pc_valid: total_ponder_cost += ponder_cost.item()
                steps_in_epoch += 1


            if (i + 1) % args.accumulation_steps == 0:
                if backward_called_in_cycle:
                    # LTM Update (Needs careful handling with PEFT)
                    ltm_grads = None
                    if outputs.get("topk_vals") is not None and outputs["topk_vals"].requires_grad and outputs["topk_vals"].grad_fn is not None:
                        if outputs["topk_vals"].grad is not None:
                            ltm_grads = outputs["topk_vals"].grad

                    if ltm_grads is not None:
                        # Access the base model's LTM module directly
                        base_ltm = model.base_model.model.ltm # Deeper nesting for PeftModel
                        ltm_grads_copy = ltm_grads.detach().clone() # Use a copy

                        if use_amp:
                            current_scale = scaler.get_scale()
                            if current_scale != 1.0 and scaler._enabled and scaler._scale is not None:
                                assert current_scale > 0.0
                                ltm_grads_copy = ltm_grads_copy / current_scale
                            elif current_scale != 1.0:
                                print(f"\nWarning: Scaler state inconsistent at step {i+1}, cannot unscale LTM grads.")

                        base_ltm.inner_update(outputs["topk_idx"], ltm_grads_copy, current_lr=args.ltm_lr, source=LTMModule.SRC_TRAINING_DATA)

                    # Optimizer Step
                    if use_amp:
                        scaler.unscale_(optimizer)
                        if args.grad_clip > 0:
                            # Only clip trainable parameters (those adapted by LoRA + saved modules like LTM)
                            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), args.grad_clip)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        if args.grad_clip > 0:
                            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), args.grad_clip)
                        optimizer.step()

                    if scheduler:
                        scheduler.step()

                    optimizer.zero_grad(set_to_none=True)
                    backward_called_in_cycle = False
                    global_step += 1
                else:
                    print(f"\nWarning: Skipping optimizer step at batch {i+1} due to invalid loss(es) in accumulation cycle.")
                    optimizer.zero_grad(set_to_none=True)
                    backward_called_in_cycle = False

            # Use steps_in_epoch for averaging
            avg_loss = total_loss / steps_in_epoch if steps_in_epoch > 0 else 0.0
            avg_ponder = total_ponder_cost / steps_in_epoch if steps_in_epoch > 0 else 0.0
            current_lr = scheduler.get_last_lr()[0] if scheduler else args.starting_lr
            pbar.set_postfix({
                "loss": f"{avg_loss:.4f}",
                "ponder": f"{avg_ponder:.2f}",
                "lr": f"{current_lr:.2e}"
            })

    print(f"Saving LoRA adapter to {args.out_dir}")
    model.save_pretrained(args.out_dir)
    # Note: Only the adapter (+ saved modules like LTM) is saved here.
    # Save tokenizer too for completeness
    try:
        if tokenizer:
            tokenizer.save_pretrained(args.out_dir)
            print(f"Tokenizer files saved to {args.out_dir}")
    except Exception as e:
        print(f"Warning: Failed to save tokenizer with adapter. Error: {e}")


# <<< MERGE LORA Function >>>
# ... (merge_lora remains unchanged) ...
def merge_lora(args, device, tokenizer):
    if not _HAS_PEFT: raise ImportError("Please install 'peft' for merging.")
    print("Running in MERGE-LORA mode...")

    print(f"Loading base model from {args.model_path}...")
    base_model, _ = load_full_model_with_config(args.model_path, device)

    print(f"Loading LoRA adapter from {args.lora_adapter_path}...")
    # Load adapter onto the base model
    try:
        model = PeftModel.from_pretrained(base_model, args.lora_adapter_path)
    except Exception as e:
        print(f"Error loading PEFT model: {e}")
        print("Ensure the adapter path is correct and compatible with the base model.")
        sys.exit(1)


    print("Merging adapter into the base model...")
    try:
        # merge_and_unload returns the merged base model
        model = model.merge_and_unload()
    except Exception as e:
        print(f"Error merging LoRA adapter: {e}")
        sys.exit(1)

    # Save the merged model to a new, self-contained directory
    os.makedirs(args.out_dir, exist_ok=True)
    output_path = os.path.join(args.out_dir, MODEL_WEIGHTS_NAME)
    print(f"Saving merged model to {output_path}...")
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': dict(model.config) # Save config from the (now merged) model
    }, output_path)

    # Copy tokenizer files from the original base model directory (or specified tokenizer path)
    tokenizer_source_path = args.tokenizer_path if args.tokenizer_path else args.model_path
    if tokenizer_source_path:
        try:
            # Reload tokenizer if necessary, or use the one passed in
            if tokenizer is None:
                print(f"Loading tokenizer from {tokenizer_source_path} to save with merged model...")
                tokenizer_to_save = AutoTokenizer.from_pretrained(tokenizer_source_path, trust_remote_code=True)
            else:
                tokenizer_to_save = tokenizer

            tokenizer_to_save.save_pretrained(args.out_dir)
            print(f"Tokenizer files saved to {args.out_dir}")
        except Exception as e:
            print(f"Warning: Could not save tokenizer files from {tokenizer_source_path}: {e}")
    else:
        print("Warning: No tokenizer source path found, cannot save tokenizer with merged model.")


    print("Merge complete.")

# <<< QUANTIZE Function >>>
# ... (quantize remains unchanged) ...
def quantize(args, device, model=None, tokenizer=None, out_dir=None):
    if not _HAS_KERNEL:
        print("ERROR: Cannot quantize model - C++ kernel not found or failed to import.")
        return

    print(f"Running in QUANTIZE mode with {args.qtype} precision...")

    # Allow passing in an already-loaded model (e.g., from train --quantize-on-complete)
    if model is None or tokenizer is None:
        if not args.model_path:
            raise ValueError("--model-path is required for quantize mode when model/tokenizer not provided.")
        print(f"Loading full-precision model from {args.model_path}...")
        # Tokenizer is loaded from the same directory as the model
        try:
            tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
            model, _ = load_full_model_with_config(args.model_path, device)
        except Exception as e:
            print(f"Error loading model or tokenizer from {args.model_path}: {e}")
            traceback.print_exc()
            sys.exit(1)

    # Ensure model is on CPU before quantization
    if model.device != torch.device('cpu'):
        print("Moving model to CPU for quantization...")
        model.cpu()

    # Determine output directory
    if out_dir is None:
        if not args.out_dir:
            # Default to creating a new dir next to the source, e.g., './my_model-INT4'
            source_dir = args.model_path if args.model_path else "./hierarchos_model"
            out_dir = source_dir.rstrip('/\\') + f"-{args.qtype}"
        else:
            out_dir = args.out_dir

    export_and_quantize_model(out_dir, model, tokenizer, qtype=args.qtype)


# <<< MODIFIED: Signal handling setup >>>
_interrupt_flag = False
_original_sigint_handler = None

def _handle_interrupt(sig, frame):
    """Sets the interrupt flag when SIGINT (Ctrl+C) is received."""
    global _interrupt_flag
    if not _interrupt_flag: # Prevent multiple prints if Ctrl+C is held
        print("\n[Interrupt received. Finishing current generation... Press Ctrl+C again to force exit.]", flush=True)
        _interrupt_flag = True
    else:
        # If interrupted again, restore original handler and exit
        print("\n[Forcing exit...]", flush=True)
        if _original_sigint_handler:
            signal.signal(signal.SIGINT, _original_sigint_handler)
        sys.exit(1)

# <<< CHAT Function (Modified) >>>
def is_positive_feedback(text: str) -> bool:
    """Checks if the user input looks like positive validation."""
    text = text.lower().strip()
    positive_triggers = {
        "good", "great", "correct", "yes", "nice", "cool", "perfect", 
        "thanks", "thx", "+", "right", "accurate"
    }
    # Check exact match or starts with (e.g., "Good job")
    if text in positive_triggers:
        return True
    first_word = text.split(' ')[0] if ' ' in text else text
    # Remove punctuation from first word for check (e.g. "Good," -> "good")
    first_word = ''.join(c for c in first_word if c.isalnum())
    return first_word in positive_triggers or text.startswith("/learn")
def chat(args, device, tokenizer):
    print("Running in CHAT mode...")

    # <<< MODIFIED: Setup signal handler >>>
    global _interrupt_flag, _original_sigint_handler
    _interrupt_flag = False # Reset flag at start
    _original_sigint_handler = signal.getsignal(signal.SIGINT)
    try:
        signal.signal(signal.SIGINT, _handle_interrupt)
    except ValueError as e: # Handle potential issues registering handler (e.g., non-main thread)
        print(f"Warning: Could not set SIGINT handler: {e}. Ctrl+C interrupt may not work gracefully.")
        _original_sigint_handler = None # Ensure we don't try to restore a non-existent handler

    model = None
    shadow_model = None
    config = None
    is_quantized = False
    inference_device = "cpu" # Default for quantized models
    ltm_has_been_updated = False # Flag to track if we need to save
    
    # <<< NEW: Buffer for Anti-Echo-Chamber Logic >>>
    # We store the PREVIOUS turn here. We only train on it if the CURRENT turn is positive.
    pending_training_data = None 

    # Determine if loading quantized or full model from the same --model-path
    if not args.model_path or not os.path.isdir(args.model_path):
        print(f"Error: Model directory not found or invalid at {args.model_path}")
        sys.exit(1)

    try:
        npz_files = [f for f in os.listdir(args.model_path) if f.endswith('.npz')]
    except FileNotFoundError:
        print(f"Error: Model directory not found at {args.model_path}")
        sys.exit(1)

    if npz_files:
        if not _HAS_KERNEL:
            print("ERROR: Cannot run quantized chat without the C++ kernel (not found or failed to import).")
            return
        model, config = load_quantized(args.model_path)
        is_quantized = True
        print(f"Loaded quantized model with {model.qtype} weights.")

        if args.device == 'vulkan':
            if not _HAS_VULKAN:
                print("WARNING: Vulkan support not found in kernel. Falling back to CPU.")
            else:
                inference_device = "vulkan"
                print("INFO: Using Vulkan for inference.")
        else: # Default to CPU if not Vulkan
            inference_device = "cpu"


        if args.enable_quantized_learning:
            if not args.shadow_model_path:
                raise ValueError("To enable learning on a quantized model, you must provide the original full-precision model directory via --shadow-model-path.")
            print("Loading full-precision 'shadow' model for online learning...")
            # Shadow model is loaded with its own config, which should match the quantized one
            try:
                # Load shadow model onto the main device (e.g., CPU if no CUDA)
                shadow_model, shadow_config = load_full_model_with_config(args.shadow_model_path, device)
                # Basic config check
                if shadow_config.context_dim != config.context_dim or shadow_config.ltm_slots != config.ltm_slots:
                    print("Warning: Shadow model config differs significantly from quantized config. Learning might be unstable.")
                # Ensure shadow model max_length matches quantized model
                if shadow_config.max_length != config.max_length:
                     print(f"Warning: Shadow model max_length ({shadow_config.max_length}) differs from quantized ({config.max_length}). Syncing shadow.")
                     shadow_config.max_length = config.max_length
                     shadow_model.config.max_length = config.max_length
                     shadow_model.pos_emb = nn.Embedding(config.max_length, shadow_model.config.context_dim).to(device)

            except Exception as e:
                print(f"Error loading shadow model from {args.shadow_model_path}: {e}")
                traceback.print_exc()
                sys.exit(1)


            # Sync the quantized model's initial LTM state to the shadow model
            shadow_model.ltm.load_state_dict(model.ltm.state_dict())
            shadow_model.eval()

    else: # Load full precision model
        try:
            model, config = load_full_model_with_config(args.model_path, device)
            inference_device = device # Use the main PyTorch device
        except Exception as e:
            print(f"Error loading full precision model from {args.model_path}: {e}")
            traceback.print_exc()
            sys.exit(1)


    if args.ltm_lora_path:
        print(f"LTM online learning is ACTIVE. Updates will be stored separately at: {args.ltm_lora_path}")
        # The model to update is the shadow model if it exists, otherwise the base model
        updatable_model = shadow_model if is_quantized and args.enable_quantized_learning else model
        if updatable_model is None:
            print("Warning: LTM LoRA path specified but no updatable model found (e.g., quantized without learning enabled). Updates will NOT be saved.")
        else:
            updatable_model.ltm.accumulate_deltas = True
            if os.path.exists(args.ltm_lora_path):
                print("Loading existing LTM deltas...")
                try:
                    deltas = torch.load(args.ltm_lora_path)
                    # Apply loaded deltas to the LTM values and the delta accumulator
                    updatable_model.ltm.vals.data.add_(deltas.to(updatable_model.ltm.vals.device))
                    updatable_model.ltm.ltm_deltas.data = deltas.to(updatable_model.ltm.ltm_deltas.device)
                    # If quantized, sync the now-updated shadow LTM back to the live model
                    if is_quantized and args.enable_quantized_learning:
                        model.ltm.load_state_dict(updatable_model.ltm.state_dict())
                except Exception as e:
                    print(f"Warning: Failed to load or apply LTM deltas from {args.ltm_lora_path}: {e}")

    elif not is_quantized or args.enable_quantized_learning:
        print("LTM online learning is ACTIVE. Updates will modify model weights directly in memory.")

    if not is_quantized:
        model.eval()

    ltm_scheduler = None
    # Setup LTM scheduler if not in static mode and learning is enabled
    if not args.static_ltm_lr and (not is_quantized or args.enable_quantized_learning):
        print("INFO: Using Cosine Annealing schedule for LTM updates.")
        print(f"         - Max LR: {args.ltm_lr:.2e}, Min LR: {args.ltm_schedule_min_lr:.2e}, Cycle Steps: {args.ltm_schedule_steps}")
        # Schedulers need an optimizer, so we create a dummy one for the LTM LR.
        dummy_param = nn.Parameter(torch.tensor(0.0)) # Needs to be Parameter
        # Use the main LTM LR as the MAX LR for the schedule
        ltm_optimizer = torch.optim.SGD([dummy_param], lr=args.ltm_lr)
        ltm_scheduler = CosineAnnealingLR(
            ltm_optimizer,
            T_max=args.ltm_schedule_steps,
            eta_min=args.ltm_schedule_min_lr
        )

    # Initialize AMP scaler and dummy optimizer for chat learning
    scaler = None
    dummy_optimizer = None
    # Enable AMP for learning if requested AND possible (CUDA available AND (full model OR quantized learning enabled))
    # <<< MODIFIED: Changed device check >>>
    use_amp = args.amp and _HAS_AMP and (not is_quantized or args.enable_quantized_learning) and (device.type == 'cuda')

    if use_amp:
        scaler = GradScaler()
        # Create a dummy optimizer for the scaler to track state (NaNs/Infs)
        dummy_param_amp = nn.Parameter(torch.tensor(0.0)).to(device) # Needs to be Parameter and on device
        dummy_optimizer = torch.optim.SGD([dummy_param_amp], lr=1.0) # Dummy optimizer for AMP scaler
        print("INFO: Automatic Mixed Precision (AMP) ENABLED for online learning.")

    print("\nWelcome to hierarchos Chat. Type 'exit' or 'quit' to end.")
    print("Use '/filter time=-<seconds>' or '/filter source=<id>' to constrain memory.")
    print("Example: /filter time=-3600  (memories from the last hour)")
    print("Use '/filter reset' to clear memory filters.")
    # <<< MODIFIED: Updated interrupt message >>>
    print("Press Ctrl+C to stop generation at any time.")
    print("="*50)

    try:
        min_ts_filter = 0.0
        source_id_filter = None
        while True:
            # <<< MODIFIED: Reset interrupt flag before getting input >>>
            _interrupt_flag = False
            try:
                prompt = input(">>> ")
            except EOFError: # Handle case where input stream ends unexpectedly
                print("\n[EOF detected. Exiting chat.]")
                break

            if prompt.lower() in ["exit", "quit"]:
                break

            # Simple command parser for filtering
            if prompt.startswith('/filter'):
                parts = prompt.split()
                try:
                    if len(parts) == 1 or parts[1] == 'reset':
                        min_ts_filter = 0.0
                        source_id_filter = None
                        print("[INFO: Memory filters have been reset.]")
                        continue
                    for part in parts[1:]:
                        if '=' not in part: raise ValueError(f"Invalid filter format: {part}")
                        key, value = part.split('=', 1)
                        if key == 'time':
                            time_offset = float(value)
                            if time_offset <= 0:
                                min_ts_filter = time.time() + time_offset # Add negative offset
                                print(f"[INFO: Memory filtered to events after {time.ctime(min_ts_filter)}]")
                            else:
                                print("[ERROR: Time filter requires a negative offset, e.g., time=-3600]")
                        elif key == 'source':
                            src_id = int(value)
                            if src_id in [LTMModule.SRC_UNKNOWN, LTMModule.SRC_USER_INTERACTION, LTMModule.SRC_TRAINING_DATA]:
                                source_id_filter = src_id
                                print(f"[INFO: Memory filtered to source ID: {source_id_filter}]")
                            else:
                                print(f"[ERROR: Invalid source ID. Use {LTMModule.SRC_UNKNOWN}, {LTMModule.SRC_USER_INTERACTION}, or {LTMModule.SRC_TRAINING_DATA}]")
                        else:
                            print(f"[ERROR: Unknown filter key: {key}]")
                except Exception as e: # Catch broader errors like split issues
                    print(f"[ERROR: Invalid filter format. Use 'time=-<seconds>' or 'source=<id>'. Details: {e}]")
                continue
            
            # =================================================================
            # A. CHECK FOR FEEDBACK & PERFORM DELAYED UPDATE (Anti-Echo-Chamber)
            # =================================================================
            if pending_training_data is not None:
                should_learn = is_positive_feedback(prompt) or prompt.strip() == "/learn"
                
                if should_learn:
                    p_ids = pending_training_data['prompt_ids']
                    r_ids = pending_training_data['response_ids']
                    
                    # Select appropriate model (shadow or base)
                    update_model = shadow_model if is_quantized else model
                    target_device = device

                    print("[Feedback detected. Updating LTM...]", end="", flush=True)

                    update_model.train()
                    with torch.enable_grad():
                        # Reconstruct sequence
                        full_sequence = torch.cat([p_ids[0], r_ids], dim=0).unsqueeze(0)
                        labels = torch.cat([torch.full_like(p_ids[0], -100), r_ids], dim=0).unsqueeze(0)
                        
                        if full_sequence.shape[1] > config.max_length:
                             full_sequence = full_sequence[:, -config.max_length:]
                             labels = labels[:, -config.max_length:]

                        # Zero grads
                        if use_amp: dummy_optimizer.zero_grad(set_to_none=True)
                        update_model.zero_grad(set_to_none=True)

                        # Forward / Backward
                        with autocast(device_type=target_device.type, enabled=use_amp):
                            outputs = update_model(input_ids=full_sequence, labels=labels)
                            loss = outputs["loss"]
                            combined_loss = loss 

                        if combined_loss is not None:
                            if use_amp:
                                scaler.scale(combined_loss).backward()
                            else:
                                combined_loss.backward()
                            
                            ltm_grads = None
                            if outputs.get("topk_vals") is not None and outputs["topk_vals"].grad is not None:
                                ltm_grads = outputs["topk_vals"].grad

                            if ltm_grads is not None:
                                ltm_grads_copy = ltm_grads.detach().clone()
                                
                                # Handle LR
                                current_ltm_lr = args.ltm_lr
                                if ltm_scheduler:
                                    current_ltm_lr = ltm_scheduler.get_last_lr()[0]
                                    ltm_scheduler.step()

                                # Handle AMP Unscale
                                if use_amp:
                                    current_scale = scaler.get_scale()
                                    if current_scale != 1.0:
                                        ltm_grads_copy = ltm_grads_copy / current_scale
                                
                                # --- UPDATE LTM ---
                                update_model.ltm.inner_update(
                                    outputs["topk_idx"], 
                                    ltm_grads_copy, 
                                    current_lr=current_ltm_lr, 
                                    source=LTMModule.SRC_USER_INTERACTION
                                )
                                ltm_has_been_updated = True
                                
                                # Clean up AMP scaler state
                                if use_amp:
                                    scaler.unscale_(dummy_optimizer)
                                    scaler.step(dummy_optimizer)
                                    scaler.update()
                                    
                                update_model.zero_grad(set_to_none=True)
                                
                                # Sync back to quantized model if needed
                                if is_quantized:
                                    model.ltm.load_state_dict(update_model.ltm.state_dict())
                                    
                                print(f" Done. (Loss: {loss.item():.3f})")
                            else:
                                print(" (No LTM gradients generated)")
                        else:
                            print(" (Loss invalid, skipped)")
                    
                    update_model.eval()
                else:
                    # User did not approve. The pending data is essentially discarded here.
                    pass 

            # Clear pending data after processing logic
            pending_training_data = None

            # If user typed strictly "/learn" to trigger update, don't generate a response to "/learn"
            if prompt.strip() == "/learn":
                print("[Ready for next input]")
                continue


            # =================================================================
            # B. GENERATION LOGIC
            # =================================================================
            
            # Kayla format assumes ### wrappers
            prompt_format = f"### Instruction:\n{prompt}\n\n### Response:\n"

            # Always use the main device for initial tokenization
            if tokenizer is None:
                print("Error: Tokenizer not loaded. Cannot proceed.")
                break
            prompt_ids = tokenizer.encode(prompt_format, return_tensors="pt").to(device)


            print("\nhierarchos: ", end="", flush=True)
            response_ids = []

            # Initialize RNN states on the appropriate device
            rnn_device = "cpu" if is_quantized else device
            h_state = torch.zeros(1, config.h_hidden, device=rnn_device)
            l_state = torch.zeros(1, config.l_hidden, device=rnn_device)

            current_ids = prompt_ids
            # Generation loop uses no_grad
            with torch.no_grad():
                for i in range(args.max_new_tokens):
                    # <<< MODIFIED: Check signal interrupt flag >>>
                    if _interrupt_flag:
                        _interrupt_flag = False # Reset flag for next turn
                        print("\n[Generation interrupted by user.]", end="", flush=True)
                        break

                    # Input for the model call
                    # Pass CPU tensors to quantized model, CUDA/MPS to full model
                    model_input_ids = current_ids.cpu() if is_quantized else current_ids.to(device)


                    if is_quantized:
                        # Quantized model expects CPU input ids and states
                        outputs = model(model_input_ids, h_state.cpu(), l_state.cpu(), device=inference_device, min_timestamp=min_ts_filter, source_filter=source_id_filter)
                        # Update CPU states
                        h_state, l_state = outputs['h_state'], outputs['l_state']
                    else:
                        # Full model expects inputs on its device
                        outputs = model(model_input_ids.to(device), min_timestamp=min_ts_filter, source_filter=source_id_filter)
                        # State is handled internally by the full model's forward pass


                    logits = outputs["logits"].to(device) # Ensure logits are on main device for sampling
                    next_token_logits = logits[:, -1, :]

                    # Simple argmax sampling (can be replaced with more sophisticated sampling)
                    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)


                    if next_token_id.item() == tokenizer.eos_token_id:
                        break

                    response_ids.append(next_token_id.item())
                    # Decode token safely, handling potential errors
                    try:
                        decoded_token = tokenizer.decode([next_token_id.item()])
                    except Exception as e:
                        print(f"[Decode Error: {e}]", end="")
                        decoded_token = "" # Continue with empty token


                    # Stop generation if a special token like ### is encountered
                    if "###" in decoded_token and len(decoded_token) <= 5:
                        break

                    print(decoded_token, end="", flush=True)

                    # Append the new token for the next iteration's input
                    current_ids = torch.cat([current_ids, next_token_id.to(current_ids.device)], dim=1)
                    # Truncate input_ids if they exceed max_length (important for long chats)
                    if current_ids.shape[1] > config.max_length:
                        current_ids = current_ids[:, -config.max_length:]

            print("\n")

            # =================================================================
            # C. BUFFER DATA FOR NEXT TURN
            # =================================================================
            # Instead of learning immediately, we buffer.
            learning_enabled = not is_quantized or args.enable_quantized_learning
            if len(response_ids) > 0 and learning_enabled:
                 pending_training_data = {
                    'prompt_ids': prompt_ids,
                    'response_ids': torch.tensor(response_ids, device=device)
                 }

    except KeyboardInterrupt:
        print("\n\n[Ctrl+C detected. Exiting chat.]")

    finally:
        # <<< MODIFIED: Restore original signal handler >>>
        if _original_sigint_handler:
            signal.signal(signal.SIGINT, _original_sigint_handler)

        # --- SAVE ON EXIT LOGIC ---
        updatable_model = shadow_model if is_quantized and args.enable_quantized_learning else model
        # Check if updatable_model is valid before proceeding
        can_update = updatable_model is not None and (not is_quantized or args.enable_quantized_learning)

        if can_update and args.ltm_lora_path and hasattr(updatable_model.ltm, 'accumulate_deltas') and updatable_model.ltm.accumulate_deltas:
            # Save accumulated deltas if they exist
            if torch.any(updatable_model.ltm.ltm_deltas != 0):
                print(f"\nSaving LTM memory deltas to {args.ltm_lora_path}...")
                try:
                    torch.save(updatable_model.ltm.ltm_deltas.cpu(), args.ltm_lora_path)
                    print("✅ Deltas saved.")
                except Exception as e:
                    print(f"Error saving LTM deltas: {e}")
            else:
                print("\nNo new LTM updates to save as LoRA.")

        elif can_update and not args.ltm_lora_path and ltm_has_been_updated:
            # Prompt to save directly incorporated updates
            if not is_quantized: # Save full precision model directly
                while True:
                    try: # Handle potential EOFError in some environments
                        response = input(f"Do you want to save the learned LTM updates back to '{args.model_path}'? (y/n): ").lower()
                        if response in ["y", "yes"]:
                            print(f"\nSaving updated model to {args.model_path}...")
                            output_weights_path = os.path.join(args.model_path, MODEL_WEIGHTS_NAME)
                            try:
                                torch.save({
                                    'model_state_dict': model.state_dict(),
                                    'config': dict(model.config) # Save current config
                                }, output_weights_path)
                                print("✅ Save complete.")
                            except Exception as e:
                                print(f"Error saving model: {e}")
                            break
                        elif response in ["n", "no"]:
                            print("Changes will be discarded. Exiting.")
                            break
                        else:
                            print("Invalid input.")
                    except EOFError:
                        print("\nEOF detected. Assuming 'no' for saving.")
                        break
            else:
                # Re-quantize prompt
                output_dir = args.model_path # Overwrite the existing quantized model dir
                while True:
                    try: # Handle potential EOFError
                        response = input(f"Do you want to save these LTM changes by re-quantizing the model to '{output_dir}'? (y/n): ").lower()
                        if response in ["y", "yes"]:
                            print(f"\nRe-quantizing model with updated LTM to {output_dir}...")
                            try:
                                # Ensure the shadow model is on CPU for quantization if needed
                                shadow_model.cpu()
                                # Make sure tokenizer is available for export
                                if tokenizer is None:
                                    print("Error: Cannot re-quantize without a loaded tokenizer.")
                                    break
                                export_and_quantize_model(output_dir, shadow_model, tokenizer, model.qtype)
                                print("✅ Re-quantization complete.")
                                # Move shadow model back to original device if needed
                                shadow_model.to(device)
                            except Exception as e:
                                print(f"Error during re-quantization: {e}")
                                traceback.print_exc()
                            break
                        elif response in ["n", "no"]:
                            print("Changes will be discarded. Exiting.")
                            break
                        else:
                            print("Invalid input.")
                    except EOFError:
                        print("\nEOF detected. Assuming 'no' for re-quantizing.")
                        break

        elif ltm_has_been_updated:
             print("\n[Warning] LTM was updated, but no valid save configuration was found. Changes lost.")
def main():
    parser = argparse.ArgumentParser(description="hierarchos: A Hybrid Memory-Reasoning Architecture")
    parser.add_argument("mode", type=str, choices=["train", "finetune", "chat", "quantize", "merge-lora"], help="Operation mode.")

    # --- Data and Path Arguments (Universal) ---
    path_group = parser.add_argument_group('Paths and Data')
    # <<< MODIFIED: --train now uses nargs='?' to be optional without a value >>>
    path_group.add_argument("--train", type=str, nargs='?', default=None, const=True, help="[Train/Finetune] Path to local training data: JSON/JSONL file, or directory for pre-chunked .pt tensors. If used without a path, requires --hf_dataset.")
    # <<< NEW: Hugging Face dataset arguments >>>
    path_group.add_argument("--hf_dataset", type=str, default=None, help="[Train/Finetune] Name or path to a Hugging Face dataset (e.g., 'wikitext', 'c4', 'path/to/my_csv/').")
    path_group.add_argument("--hf_dataset_config", type=str, default=None, help="[Train/Finetune] Optional configuration name for the HF dataset (e.g., 'wikitext-103-raw-v1' for wikitext).")
    path_group.add_argument("--hf_dataset_split", type=str, default="train", help="[Train/Finetune] Dataset split to use (e.g., 'train', 'validation', 'train[:10%%]').")
    # <<< NEW: Column name arguments for HF datasets >>>
    path_group.add_argument("--text_column", type=str, default=None, help="[Train/Finetune] Column name for text completion data in HF dataset (mutually exclusive with prompt/completion). Defaults to 'text' if available.")
    path_group.add_argument("--prompt_column", type=str, default=None, help="[Train/Finetune] Column name for prompt/instruction in HF dataset.")
    path_group.add_argument("--completion_column", type=str, default=None, help="[Train/Finetune] Column name for completion/response in HF dataset.")
    # --- Existing path arguments ---
    path_group.add_argument("--model-path", type=str, default=None, help="Path to the model directory (required for all modes except 'train' unless resuming or starting from scratch).")
    path_group.add_argument("--out-dir", type=str, default="./hierarchos_model", help="[Train/Finetune/Merge/Quantize] Directory to save the new model/adapter.")
    path_group.add_argument("--lora-adapter-path", type=str, default=None, help="[Merge/Finetune] Path to the LoRA adapter directory.")
    path_group.add_argument("--tokenizer-path", type=str, default=None, help="Path or HF name of the tokenizer (used if not loading from model-path, defaults to openai-community/gpt2).") # Allow None default
    path_group.add_argument("--resume-from-ckpt", type=str, default=None, help="[Train] Path to a specific training checkpoint .pt file to resume from.")
    path_group.add_argument("--shadow-model-path", type=str, default=None, help="[Chat] Path to the original full-precision model dir, required for online learning with a quantized model.")


    # <<< MODIFIED: Added --pre_pt_dataset flag >>>
    data_fmt_group = parser.add_mutually_exclusive_group()
    data_fmt_group.add_argument("--pre_chunked_dataset", action="store_true", help="[Train/Finetune] If set, assumes --train points to a pre-tokenized/chunked/padded JSONL (IterableDataset). Requires --max_length.")
    data_fmt_group.add_argument("--pre_pt_dataset", action="store_true", help="[Train/Finetune] If set, assumes --train points to a directory with pre-chunked .pt tensor files and manifest.jsonl (Map-Style Dataset). Requires --max_length.")


    # --- Model Architecture Arguments (for Training) ---
    arch_group = parser.add_argument_group('Architecture (for --mode train, used if not resuming/loading)')
    # <<< MODIFIED: Changed some defaults >>>
    arch_group.add_argument("--context_dim", type=int, default=768) # Default for GPT-2 small
    arch_group.add_argument("--persistent_dim", type=int, default=128)
    arch_group.add_argument("--ltm_slots", type=int, default=1024)
    arch_group.add_argument("--ltm_key_dim", type=int, default=128)
    arch_group.add_argument("--ltm_val_dim", type=int, default=128)
    arch_group.add_argument("--h_hidden", type=int, default=768) # Match context_dim
    arch_group.add_argument("--l_hidden", type=int, default=768) # Match context_dim
    arch_group.add_argument("--max_h_steps", type=int, default=5, help="[HRM] Maximum number of high-level refinement steps.")
    arch_group.add_argument("--max_l_steps", type=int, default=5, help="[HRM] Maximum number of low-level iterations before forcing completion.")
    arch_group.add_argument("--l_conv_atol", type=float, default=1e-4, help="[HRM] Absolute tolerance for checking L-module state convergence.")
    arch_group.add_argument("--ltm_topk", type=int, default=2, help="Number of LTM slots to retrieve per token.")
    arch_group.add_argument("--max_length", type=int, default=1024, help="Max sequence length. Required if using --pre_chunked_dataset, --pre_pt_dataset. Defaults to 1024 if not loading a model config or using --auto-max-length.") # <<< Modified default and help text
    arch_group.add_argument("--auto-max-length", action="store_true", help="Automatically scan the dataset (--train or --hf_dataset) to find the longest sequence and set it as max_length.")

    # --- Training Arguments ---
    train_group = parser.add_argument_group('Training and Finetuning')
    train_group.add_argument("--epochs", type=int, default=3)
    train_group.add_argument("--batch_size", type=int, default=4)
    train_group.add_argument("--accumulation-steps", type=int, default=1, help="Simulates a larger batch size.")
    train_group.add_argument("--starting-lr", type=float, default=1e-4)
    train_group.add_argument("--min-lr", type=float, default=1e-6, help="Min LR for cosine annealing.")
    train_group.add_argument("--disable-lr-schedule", action="store_true", help="Use a fixed LR instead of cosine annealing.")
    train_group.add_argument("--ltm_lr", type=float, default=1e-2, help="[Static] LR for LTM updates, or [Scheduled] MAX LR for the LTM cosine schedule.")
    train_group.add_argument("--kayla", action="store_true", help="Enable Kayla-style instruction tuning (with thought-process). Ignored if using pre-chunked formats or --text_column.")
    train_group.add_argument("--lora_r", type=int, default=8, help="[Finetune] LoRA rank.")
    train_group.add_argument("--lora_alpha", type=int, default=16, help="[Finetune] LoRA alpha.")
    train_group.add_argument("--finetune-unlock-percent", type=float, default=None, help="[Finetune] Target percentage of params to train (e.g., 1.5 for 1.5%). Overrides --lora_r.")
    train_group.add_argument("--quantize-on-complete", action="store_true", help="[Train] Automatically quantize after training.")
    train_group.add_argument("--grad-clip", type=float, default=1.0, help="Gradient clipping value. Set to 0 to disable.")
    train_group.add_argument("--ponder-loss-weight", type=float, default=0.01, help="[HRM] Weight for the ponder cost auxiliary loss.")
    train_group.add_argument("--override-scheduling", action="store_true", help="[Train] If resuming, ignore the scheduler state in the checkpoint and use the new LR args.")
    train_group.add_argument("--num_workers", type=int, default=0, help="Number of worker processes for data loading. Recommended: 2 or 4 for GPU training.")
    train_group.add_argument("--amp", action="store_true", help="[Train/Finetune/Chat] Enable Automatic Mixed Precision (AMP) for training/learning.")
    # <<< NEW: Gradient Checkpointing Flag >>>
    train_group.add_argument("--gradient-checkpointing", action="store_true",
                                help="[Train/Finetune] Enable gradient checkpointing to save memory during training.")
    # <<< END >>>


    # --- Inference Arguments ---
    infer_group = parser.add_argument_group('Inference (Chat)')
    infer_group.add_argument("--max-new-tokens", type=int, default=512)
    infer_group.add_argument("--enable-quantized-learning", action="store_true", help="[Chat] Enable LTM updates for quantized models. Requires --shadow-model-path.")
    infer_group.add_argument("--ltm-lora-path", type=str, default=None, help="[Chat] Optional: Path to save/load LTM updates as a separate delta file.")
    infer_group.add_argument("--device", type=str, default="cpu", choices=["cpu", "vulkan"], help="[Chat] Device for quantized inference.")
    infer_group.add_argument("--h-halt-thresh", type=float, default=0.9, help="[HRM] Probability threshold for early exiting the H-module loop during inference.")
    infer_group.add_argument("--static-ltm-lr", action="store_true", help="[Chat] Disable the cosine annealing schedule for LTM updates and use a fixed LR instead.")
    infer_group.add_argument("--ltm-schedule-steps", type=int, default=100, help="[Chat] The number of updates in one cosine annealing cycle for LTM learning.")
    infer_group.add_argument("--ltm-schedule-min-lr", type=float, default=1e-5, help="[Chat] The minimum learning rate for the LTM cosine annealing schedule.")

    # --- Other Arguments ---
    other_group = parser.add_argument_group('Other Settings')
    other_group.add_argument("--qtype", type=str, default="INT4", choices=["INT4", "Q4_0", "Q8_0", "Q2_K"], help="Quantization type/format.")
    other_group.add_argument("--threads", type=int, default=max(1, os.cpu_count() // 2))

    args = parser.parse_args()

    # --- Argument Validation ---
    # <<< MODIFIED: Data source validation >>>
    if args.mode in ['train', 'finetune']:
        # Check if NO data source is provided (unless resuming train)
        if not args.hf_dataset and args.train is None and not (args.mode == 'train' and args.resume_from_ckpt):
            parser.error("Either `--train path/to/local/data` or `--hf_dataset name/or/path` must be specified for train/finetune mode (unless resuming train from ckpt).")
        # Check if BOTH --train path AND --hf_dataset are given
        if args.hf_dataset and args.train is not None and args.train is not True: # Check if --train has a path argument
             parser.error("Cannot specify both `--train path/to/local/data` and `--hf_dataset` simultaneously.")
        # Ensure --train wasn't used as just a flag without --hf_dataset
        if args.train is True and not args.hf_dataset:
            parser.error("The `--train` flag was used without a path, but no `--hf_dataset` was provided.")


        if args.hf_dataset and args.pre_chunked_dataset:
            parser.error("--hf_dataset cannot be used with --pre_chunked_dataset.")
        if args.hf_dataset and args.pre_pt_dataset:
            parser.error("--hf_dataset cannot be used with --pre_pt_dataset.")
        # Removed the auto_max_length warning here, handled later.
        if args.hf_dataset and not _HAS_HF_DATASETS:
            parser.error("--hf_dataset specified, but the 'datasets' library is not installed. Please run: pip install datasets")
        if args.hf_dataset:
            if args.text_column and (args.prompt_column or args.completion_column):
                parser.error("--text_column is mutually exclusive with --prompt_column and --completion_column.")
            if (args.prompt_column and not args.completion_column) or (not args.prompt_column and args.completion_column):
                parser.error("Both --prompt_column and --completion_column must be specified together for instruction tuning.")

    # --- Existing validation ---
    if args.mode == 'finetune' and not args.model_path:
        parser.error("`--model-path` (base model) is required for finetune mode.")
    if args.mode == 'merge-lora' and not args.model_path:
        parser.error("`--model-path` (base model) is required for merge-lora mode.")
    if args.mode == 'merge-lora' and not args.lora_adapter_path:
        parser.error("`--lora-adapter-path` is required for merge-lora mode.")
    if args.mode == 'quantize' and not args.model_path and not args.quantize_on_complete:
        is_standalone_quantize = True
        for i, arg in enumerate(sys.argv):
            if arg == 'train' and '--quantize-on-complete' in sys.argv:
                is_standalone_quantize = False
                break
        if is_standalone_quantize and not args.model_path:
            parser.error("`--model-path` is required for standalone quantize mode.")
    if args.mode == 'chat' and not args.model_path:
        parser.error("`--model-path` is required for chat mode.")
    if args.enable_quantized_learning and not args.shadow_model_path:
        parser.error("--enable-quantized-learning requires --shadow-model-path to be set.")
    if (args.pre_chunked_dataset or args.pre_pt_dataset) and not args.max_length:
        parser.error("--max_length must be specified when using --pre_chunked_dataset or --pre_pt_dataset.")
    if (args.pre_chunked_dataset or args.pre_pt_dataset) and args.auto_max_length:
        print("Warning: --auto-max-length is ignored when using pre-chunked dataset formats.")
        args.auto_max_length = False # Disable it
    if (args.pre_chunked_dataset or args.pre_pt_dataset) and args.kayla:
        print("Warning: --kayla flag is ignored when using pre-chunked dataset formats.")

    set_threads(args.threads)
    pt_device = pick_device()
    print(f"Using PyTorch device: {pt_device}")

    if pt_device.type == 'cuda':
        torch.set_float32_matmul_precision('high')

    # --- AMP Availability Check ---
    if args.amp and not _HAS_AMP:
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print("!!! WARNING: --amp was specified, but torch amp support is not available.     !!!")
        print("!!!  AMP will be DISABLED. Check CUDA and PyTorch install.                  !!!")
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        args.amp = False # Force disable
    elif args.amp and pt_device == torch.device('cpu'):
        print("Warning: AMP (--amp) is enabled but running on CPU. AMP will have no effect.")
        # Don't disable args.amp, just warn

    # --- Tokenizer Loading ---
    tokenizer = None
    tokenizer_load_path = None
    default_tokenizer = "openai-community/gpt2" # <<< Changed default

    # 1. Prioritize --tokenizer-path if given
    if args.tokenizer_path:
        print(f"Attempting to load tokenizer from specified path: '{args.tokenizer_path}'...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, trust_remote_code=True)
            tokenizer_load_path = args.tokenizer_path
            print(f"Successfully loaded tokenizer from '{args.tokenizer_path}'")
        except Exception as e:
            print(f"Warning: Failed to load tokenizer from '{args.tokenizer_path}'. Error: {e}")
            # Fall through to try model_path or default

    # 2. If no tokenizer yet, try --model-path (if applicable for the mode)
    #    Load tokenizer from model_path UNLESS starting a fresh train run
    if tokenizer is None and args.model_path and not (args.mode == 'train' and not args.resume_from_ckpt and not args.model_path):
        print(f"Attempting to load tokenizer from model directory: {args.model_path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
            tokenizer_load_path = args.model_path
            print(f"Successfully loaded tokenizer from '{args.model_path}'")
        except Exception as e:
            print(f"Warning: Could not load tokenizer from model directory '{args.model_path}'. Will try default. Error: {e}")
            # Fall through to try default

    # 3. If still no tokenizer, try the default (important for fresh train)
    if tokenizer is None:
        print(f"Attempting to load default tokenizer: '{default_tokenizer}'...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(default_tokenizer, trust_remote_code=True)
            tokenizer_load_path = default_tokenizer
            print(f"Successfully loaded tokenizer from default '{default_tokenizer}'")
        except Exception as e:
            # If default fails, it's critical for modes needing it
            if args.mode in ["train", "finetune", "chat"] or (args.mode == "quantize" and not args.quantize_on_complete) or args.mode == "merge-lora":
                print(f"ERROR: Failed to load tokenizer from all potential sources (specified: '{args.tokenizer_path}', model: '{args.model_path}', default: '{default_tokenizer}'). Cannot continue.")
                print(f"Details: {e}")
                sys.exit(1)
            else:
                print(f"Warning: Tokenizer loading failed from all paths. Relying on it being passed directly later if needed.")
                tokenizer = None


    # --- Tokenizer Pad Token Handling ---
    if tokenizer and tokenizer.pad_token is None:
        if tokenizer.eos_token:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"Tokenizer missing pad token, setting pad_token = eos_token ({tokenizer.pad_token})")
        else:
            print("Warning: Tokenizer missing both pad and eos tokens. Adding a '[PAD]' token.")
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            # If adding token, vocab size changes. Model needs to know ONLY IF TRAINING FROM SCRATCH.
            if args.mode == "train" and not args.resume_from_ckpt and not args.model_path:
                # Update args directly as config isn't finalized yet
                args.vocab_size = len(tokenizer) # Set vocab_size based on modified tokenizer
                print(f"Updated args.vocab_size to {args.vocab_size} due to added pad token.")


    # --- Temporary HF Dataset Loading (for auto-max-length) ---
    # <<< NEW: Load HF dataset here if needed for scanning >>>
    hf_raw_dataset_for_scan = None
    if args.auto_max_length and args.hf_dataset and args.mode in ['train', 'finetune']:
        if tokenizer is None:
             parser.error("--auto-max-length requires the tokenizer to be loaded first.")
        if not _HAS_HF_DATASETS:
             parser.error("--auto-max-length requested for HF dataset, but 'datasets' library is not installed.")
        print(f"Loading Hugging Face dataset ({args.hf_dataset}) temporarily for length scan...")
        try:
            hf_raw_dataset_for_scan = load_dataset(args.hf_dataset, name=args.hf_dataset_config, split=args.hf_dataset_split)
            print("Dataset loaded for scan.")
        except Exception as e:
            print(f"ERROR: Failed to load HF dataset '{args.hf_dataset}' for length scan: {e}")
            sys.exit(1)


    # --- Auto Max Length Scanning (Handles Local Files and HF Datasets) ---
    # <<< MODIFIED: Reworked this section >>>
    if args.auto_max_length and args.mode in ['train', 'finetune'] and not args.pre_chunked_dataset and not args.pre_pt_dataset:
        if tokenizer is None:
            parser.error("--auto-max-length requires the tokenizer to be loaded first.")

        max_found_length = 0
        skipped_scan_count = 0
        scan_desc = "Scanning Dataset"

        if args.hf_dataset:
            # --- Scan HF Dataset ---
            if hf_raw_dataset_for_scan is None:
                 # Should have been loaded above, but double-check
                 print("ERROR: HF dataset required for scan was not loaded.")
                 sys.exit(1)
            print("INFO: --auto-max-length enabled. Scanning HF dataset...")
            scan_desc = f"Scanning HF: {args.hf_dataset}"
            for sample in tqdm(hf_raw_dataset_for_scan, desc=scan_desc):
                processed = process_text_sample(
                    tokenizer, sample, 999999, args.kayla, # Use large max_length for scanning
                    args.text_column, args.prompt_column, args.completion_column
                )
                if processed:
                    length = len(processed['input_ids'])
                    if length > max_found_length: max_found_length = length
                else: skipped_scan_count += 1

        else: # --- Scan Local JSON/JSONL File ---
            train_file_path = args.train # Should have a path if not HF
            if not train_file_path and args.resume_from_ckpt:
                # Try loading from checkpoint config if resuming and path not given
                print("INFO: --auto-max-length used with --resume-from-ckpt. Trying to load train path from config...")
                try:
                    ckpt = torch.load(args.resume_from_ckpt, map_location='cpu', weights_only=False) # Need config
                    ckpt_conf = ckpt.get('config', {})
                    train_file_path = ckpt_conf.get('train_data_path')
                except Exception as e: print(f"Warning: Could not load train path from checkpoint config: {e}")

            if not train_file_path or not os.path.exists(train_file_path):
                parser.error("--auto-max-length requires a valid --train file path (and cannot be used with --hf_dataset).")

            print(f"INFO: --auto-max-length enabled. Scanning local file: {train_file_path}...")
            scan_desc = f"Scanning Local: {os.path.basename(train_file_path)}"
            with open(train_file_path, 'r', encoding='utf-8') as f:
                try: # Try JSON list
                    f.seek(0)
                    data = json.load(f)
                    if isinstance(data, list):
                        for obj in tqdm(data, desc=scan_desc):
                            processed = process_text_sample(
                                tokenizer, obj, 999999, args.kayla,
                                prompt_column='instruction', completion_column='output' # Default keys for JSON/JSONL
                            )
                            if processed:
                                length = len(processed['input_ids'])
                                if length > max_found_length: max_found_length = length
                            else: skipped_scan_count += 1
                except: # Try JSONL
                    f.seek(0)
                    line_num_scan = 0
                    for line in tqdm(f, desc=scan_desc):
                        line_num_scan += 1
                        try:
                            obj = json.loads(line)
                            processed = process_text_sample(
                                tokenizer, obj, 999999, args.kayla,
                                prompt_column='instruction', completion_column='output'
                            )
                            if processed:
                                length = len(processed['input_ids'])
                                if length > max_found_length: max_found_length = length
                            else: skipped_scan_count += 1
                        except Exception as scan_e:
                            if skipped_scan_count % 1000 == 0:
                                print(f"\nWarning: Skipping line ~{line_num_scan} during scan: {scan_e}")
                            skipped_scan_count += 1
                            continue

        # --- Post-scan processing ---
        if skipped_scan_count > 0:
            print(f"Warning: Skipped {skipped_scan_count} invalid entries during length scan.")

        if max_found_length > 0:
            # Add a small buffer and round up to a multiple of 8 for efficiency
            target_max_length = (max_found_length + 16 + 7) & -8
            print(f"✅ Auto-scan complete. Found max length ~{max_found_length}. Setting max_length to {target_max_length}.")
            args.max_length = target_max_length # Update args for model creation/dataloader
            # Update tokenizer only if necessary
            if tokenizer and hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length < target_max_length:
                tokenizer.model_max_length = target_max_length
                print(f"Updated tokenizer.model_max_length to {target_max_length}")
        else:
            print(f"⚠️ WARNING: Auto-scan did not find valid entries or failed. Using default max_length ({args.max_length}).")
            # Keep the default argparse value if scan failed

    elif not args.max_length and args.mode in ['train', 'finetune'] and not args.pre_chunked_dataset and not args.pre_pt_dataset:
        # Case where auto not enabled, not pre-chunked, and no length given
        # Should default to 1024 based on argparse default
        print(f"Warning: No --max_length specified. Using default {args.max_length}.")


    # --- Dataloader Creation (moved here for train/finetune) ---
    dataloader = None
    dataloader_len = 0 # Length might be unknown
    if args.mode in ["train", "finetune"]:
        if tokenizer is None and not args.pre_pt_dataset and not args.hf_dataset: # Need tokenizer unless loading PT files or HF dataset
            print("Error: Tokenizer failed to load, cannot create dataloader.")
            sys.exit(1)
        if args.max_length is None: # Should be set by now unless resuming/loading model
            if not args.resume_from_ckpt and not args.model_path:
                print("Error: max_length could not be determined. Please specify --max_length or use --auto-max-length.")
                sys.exit(1)
            # If resuming/loading, max_length will be taken from config later

        try:
            if args.hf_dataset:
                # Use the dataset loaded earlier if scan happened, otherwise load now
                hf_raw_dataset = hf_raw_dataset_for_scan
                if hf_raw_dataset is None:
                     print(f"Loading Hugging Face dataset: {args.hf_dataset} (Config: {args.hf_dataset_config}, Split: {args.hf_dataset_split})")
                     hf_raw_dataset = load_dataset(args.hf_dataset, name=args.hf_dataset_config, split=args.hf_dataset_split)

                print(f"Dataset loaded: {hf_raw_dataset}")
                # <<< Ensure tokenizer is available for HF dataset >>>
                if tokenizer is None:
                     print("Error: Tokenizer is required for Hugging Face dataset processing but failed to load.")
                     sys.exit(1)
                hf_dataset = HuggingFaceMapStyleDataset(
                    hf_raw_dataset, tokenizer, args.max_length, args.kayla,
                    args.text_column, args.prompt_column, args.completion_column
                )
                dataloader = create_map_style_dataloader(
                    hf_dataset, args.batch_size, tokenizer.pad_token_id, args.num_workers, shuffle=True
                )
                dataloader_len = len(dataloader)
                print(f"INFO: HF DataLoader created with {dataloader_len} batches.")

            elif args.pre_pt_dataset:
                print("INFO: Loading pre-chunked .pt tensors (map-style).")
                dataloader = create_dataloader_pt_chunked(
                    args.train, max_length=args.max_length, batch_size=args.batch_size, num_workers=args.num_workers
                )
                dataloader_len = len(dataloader)
                print(f"INFO: DataLoader created with {dataloader_len} batches.")
            elif args.pre_chunked_dataset:
                print("INFO: Loading pre-chunked JSONL dataset (iterable).")
                dataloader = create_dataloader_for_chunked(
                    args.train, max_length=args.max_length, batch_size=args.batch_size, num_workers=args.num_workers
                )
                # Estimate length for scheduler
                try:
                    with open(args.train, 'r') as f:
                        estimated_lines = sum(1 for _ in f)
                    dataloader_len = estimated_lines // args.batch_size # Rough estimate
                    print(f"INFO: Estimated DataLoader length (for scheduler): {dataloader_len} batches.")
                except:
                    print("Warning: Could not estimate dataset length for scheduler. Using placeholder T_max=100000.")
                    dataloader_len = 100000 # Placeholder
            else: # Original JSON/JSONL loading (requires --train path)
                if not args.train or not isinstance(args.train, str):
                    parser.error("A local dataset path via --train is required when not using --hf_dataset or pre-chunked formats.")
                print("INFO: Loading and tokenizing JSON/JSONL dataset on the fly (map-style).")
                if tokenizer is None:
                     print("Error: Tokenizer is required for JSON/JSONL processing but failed to load.")
                     sys.exit(1)
                original_dataset = OriginalJSONLDataset(
                    args.train, tokenizer, args.max_length, kayla_mode=args.kayla
                )
                dataloader = create_map_style_dataloader(
                    original_dataset, args.batch_size, tokenizer.pad_token_id, args.num_workers, shuffle=True
                )
                dataloader_len = len(dataloader)
                print(f"INFO: DataLoader created with {dataloader_len} batches.")

        except Exception as e:
            print(f"ERROR creating DataLoader: {e}"); traceback.print_exc(); sys.exit(1)


    # --- Execute Selected Mode ---
    if args.mode == "train":
        train(args, pt_device, tokenizer, dataloader, dataloader_len) # Pass dataloader
    elif args.mode == "finetune":
        finetune(args, pt_device, tokenizer, dataloader, dataloader_len) # Pass dataloader
    elif args.mode == "merge-lora":
        merge_lora(args, pt_device, tokenizer)
    elif args.mode == "quantize":
        quantize(args, pt_device, tokenizer=tokenizer)
    elif args.mode == "chat":
        if tokenizer is None:
            print("Error: Tokenizer failed to load, cannot start chat.")
            sys.exit(1)
        chat(args, pt_device, tokenizer)


if __name__ == "__main__":
    # --- ADD THIS LINE ---
    # Fix for linux dataloader deadlock when num_workers > 0
    # Must be inside __name__ == "__main__" block
    try:
        torch.multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError as e:
        if "cannot be called" in str(e):
             print("INFO: Multiprocessing context already set. Skipping 'spawn'.")
        else:
             print(f"Warning: Could not set multiprocessing start method 'spawn': {e}")
    # --- END OF FIX ---
    
    main()
